{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install required packages\n!pip install -q youtube-transcript-api fpdf bitsandbytes trl peft datasets pandas nltk huggingface_hub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T05:57:54.601427Z","iopub.execute_input":"2025-05-04T05:57:54.601702Z","iopub.status.idle":"2025-05-04T05:59:20.859337Z","shell.execute_reply.started":"2025-05-04T05:57:54.601654Z","shell.execute_reply":"2025-05-04T05:59:20.858314Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.0/348.0 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Import necessary libraries for initial setup\nimport os\nimport torch\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom huggingface_hub import login\nfrom datasets import load_dataset\nfrom peft import LoraConfig, get_peft_model\nfrom trl import SFTTrainer\nfrom fpdf import FPDF\nfrom youtube_transcript_api import YouTubeTranscriptApi\nimport re\nimport nltk\nfrom nltk.tokenize import sent_tokenize\nfrom kaggle_secrets import UserSecretsClient\n\n# Download needed NLTK data\nnltk.download('punkt', quiet=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T05:59:20.860995Z","iopub.execute_input":"2025-05-04T05:59:20.861226Z","iopub.status.idle":"2025-05-04T05:59:53.484584Z","shell.execute_reply.started":"2025-05-04T05:59:20.861206Z","shell.execute_reply":"2025-05-04T05:59:53.484014Z"}},"outputs":[{"name":"stderr","text":"2025-05-04 05:59:36.207400: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746338376.499878      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746338376.581728      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# Create output directories\nOUTPUT_DIR = \"finetuned_model\"\nOUTPUTS_DIR = \"outputs\"\n\nfor directory in [OUTPUT_DIR, OUTPUTS_DIR]:\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n        print(f\"Created directory: {directory}\")\n    else:\n        print(f\"Directory already exists: {directory}\")\n\n# Authenticate with HuggingFace using Kaggle secrets\ntry:\n    hf_token = (\"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\")\n    login(token=hf_token)\n    print(\"Successfully logged in to Hugging Face\")\nexcept Exception as e:\n    print(f\"Warning: Could not log in to Hugging Face: {str(e)}\")\n    print(\"You may need to set your HUGGINGFACE_TOKEN in Kaggle secrets\")\n    print(\"Continuing without authentication (may have limited access)...\")\n\nprint(\"Setup complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T05:59:53.485344Z","iopub.execute_input":"2025-05-04T05:59:53.485558Z","iopub.status.idle":"2025-05-04T05:59:54.011140Z","shell.execute_reply.started":"2025-05-04T05:59:53.485541Z","shell.execute_reply":"2025-05-04T05:59:54.010544Z"}},"outputs":[{"name":"stdout","text":"Created directory: finetuned_model\nCreated directory: outputs\nSuccessfully logged in to Hugging Face\nSetup complete!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM, \n    BitsAndBytesConfig,\n    Trainer, \n    TrainingArguments,\n    DataCollatorForLanguageModeling\n)\nfrom datasets import load_dataset\nfrom peft import (\n    LoraConfig, \n    get_peft_model,\n    prepare_model_for_kbit_training  # Import this function from peft\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T05:59:54.012792Z","iopub.execute_input":"2025-05-04T05:59:54.013016Z","iopub.status.idle":"2025-05-04T05:59:54.016818Z","shell.execute_reply.started":"2025-05-04T05:59:54.012999Z","shell.execute_reply":"2025-05-04T05:59:54.016117Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# PART 1: FINE-TUNING GEMMA-3 ON PROGRAMMING BOOKS DATASET\n\n\n# Define paths to model and dataset\nMODEL_PATH = \"google/gemma-3-4b-it\"  # Path to Gemma model added as input\nDATASET_NAME = \"open-phi/programming_books_llama\"  # Dataset to fine-tune on\nOUTPUT_DIR = \"finetuned_model\"\n\n# Create output directory if it doesn't exist\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)\n    print(f\"Created output directory: {OUTPUT_DIR}\")\n\n\n# Load the dataset\nprint(\"Loading dataset...\")\ndataset = load_dataset(DATASET_NAME)\nprint(f\"Dataset loaded: {DATASET_NAME}\")\nprint(f\"Dataset structure: {dataset}\")\n\n# Preprocess and prepare the dataset for fine-tuning\nprint(\"Preparing dataset for fine-tuning...\")\n\ndef preprocess_function(examples):\n    \"\"\"\n    Prepare the examples for fine-tuning.\n    We'll format each example as an educational content suitable for children aged 10-16.\n    \"\"\"\n    # Initialize text list\n    texts = []\n    \n    # Get the text content from the dataset, handling different dataset structures\n    if \"text\" in examples and isinstance(examples[\"text\"], list):\n        texts = examples[\"text\"]\n    elif \"content\" in examples and isinstance(examples[\"content\"], list):\n        texts = examples[\"content\"]\n    else:\n        # Try to extract text from other fields\n        for key in examples.keys():\n            if isinstance(examples[key], list) and len(examples[key]) > 0 and isinstance(examples[key][0], str):\n                texts = examples[key]\n                break\n    \n    # If still no texts found, look for dictionary structure\n    if not texts:\n        if \"text\" in examples and not isinstance(examples[\"text\"], list):\n            texts = [examples[\"text\"]]\n        elif \"content\" in examples and not isinstance(examples[\"content\"], list):\n            texts = [examples[\"content\"]]\n        else:\n            # Return empty to avoid error\n            return {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n    \n    tokenized_inputs = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n    \n    for text in texts:\n        if not isinstance(text, str):\n            continue\n            \n        # Limit length for training efficiency\n        if len(text) > 4000:  # Truncate long texts\n            text = text[:4000]\n        \n        # Format as an educational example with a system prompt\n        formatted_text = f\"\"\"<s>\n<start_of_turn>system\nYou are an expert in creating educational content for children aged 10-16.\nYour task is to explain programming concepts in a fun, engaging way with clear examples.\nUse simple language, emojis, and clear explanations.\n<end_of_turn>\n\n<start_of_turn>user\nCreate educational content based on this information:\n{text}\n<end_of_turn>\n\n<start_of_turn>model\n\"\"\"\n        # Tokenize the formatted text\n        tokens = tokenizer(\n            formatted_text,\n            max_length=2048,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        \n        # Add to the batch\n        tokenized_inputs[\"input_ids\"].append(tokens[\"input_ids\"][0])\n        tokenized_inputs[\"attention_mask\"].append(tokens[\"attention_mask\"][0])\n        tokenized_inputs[\"labels\"].append(tokens[\"input_ids\"][0])  # Use input_ids as labels for causal LM\n    \n    # Return empty rather than error if no valid examples\n    if not tokenized_inputs[\"input_ids\"]:\n        return {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n        \n    return tokenized_inputs\n\n# Configure quantization for efficient training with limited GPU memory\nprint(\"Configuring model for fine-tuning...\")\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# Load the Gemma model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_PATH,\n    quantization_config=quantization_config,\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Configure LoRA for parameter-efficient fine-tuning\npeft_config = LoraConfig(\n    r=16,                     # Rank of update matrices\n    lora_alpha=32,            # Scaling factor\n    lora_dropout=0.05,        # Dropout probability for LoRA layers\n    bias=\"none\",              # No bias parameters\n    task_type=\"CAUSAL_LM\",    # Causal language modeling task\n    target_modules=[          # Which modules to apply LoRA to\n        \"q_proj\", \n        \"k_proj\", \n        \"v_proj\", \n        \"o_proj\",\n        \"gate_proj\", \n        \"up_proj\", \n        \"down_proj\"\n    ]\n)\n\n# Prepare model for PEFT training\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()  # Print number of trainable parameters\n\n# Apply preprocessing to the dataset\nif \"train\" in dataset:\n    processed_dataset = dataset[\"train\"].map(\n        preprocess_function,\n        batched=True,\n        batch_size=8,\n        remove_columns=dataset[\"train\"].column_names\n    )\nelse:\n    # If there's no explicit train split, use the first available split\n    first_split = list(dataset.keys())[0]\n    processed_dataset = dataset[first_split].map(\n        preprocess_function,\n        batched=True,\n        batch_size=8,\n        remove_columns=dataset[first_split].column_names\n    )\n\n# Filter out empty examples\nprocessed_dataset = processed_dataset.filter(\n    lambda example: len(example['input_ids']) > 0\n)\n\n# Take a subset of the dataset for faster fine-tuning if needed\nMAX_SAMPLES = 1000  # Adjust based on available compute resources\nif len(processed_dataset) > MAX_SAMPLES:\n    processed_dataset = processed_dataset.select(range(MAX_SAMPLES))\n\nprint(f\"Processed dataset size: {len(processed_dataset)} examples\")\n\n# Set up training arguments\nfrom transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=1,               # Number of training epochs\n    per_device_train_batch_size=1,    # Batch size per GPU\n    gradient_accumulation_steps=8,    # Number of updates steps to accumulate before backward pass\n    learning_rate=2e-4,               # Initial learning rate\n    weight_decay=0.001,               # Weight decay to reduce overfitting\n    logging_steps=10,                 # Log training metrics every X steps\n    save_steps=100,                   # Save checkpoint every X steps\n    max_steps=500,                    # Max number of training steps (override epochs if specified)\n    warmup_steps=50,                  # Linear warmup over warmup_steps\n    lr_scheduler_type=\"cosine\",       # Learning rate scheduler type\n    bf16=False,                       # Use bfloat16 precision if available\n    fp16=True,                        # Use fp16 precision\n    report_to=\"none\",                 # Disable wandb reporting to avoid errors\n)\n\n# Define data collator\nfrom transformers import DataCollatorForLanguageModeling\n\n# Use the DataCollatorForLanguageModeling\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False  # Not using masked language modeling\n)\n\n# Initialize the standard Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=processed_dataset,\n    data_collator=data_collator,\n)\n\n# Fine-tune the model\nprint(\"Starting fine-tuning...\")\ntrainer.train()\n\n# Save the fine-tuned model\nprint(\"Fine-tuning complete! Saving model...\")\ntrainer.model.save_pretrained(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\nprint(f\"Model saved to {OUTPUT_DIR}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T04:48:54.354453Z","iopub.execute_input":"2025-05-03T04:48:54.355065Z","iopub.status.idle":"2025-05-03T11:40:15.052864Z","shell.execute_reply.started":"2025-05-03T04:48:54.355040Z","shell.execute_reply":"2025-05-03T11:40:15.051877Z"}},"outputs":[{"name":"stdout","text":"Loading dataset...\nDataset loaded: open-phi/programming_books_llama\nDataset structure: DatasetDict({\n    train: Dataset({\n        features: ['topic', 'outline', 'concepts', 'queries', 'context', 'markdown', 'model'],\n        num_rows: 111048\n    })\n})\nPreparing dataset for fine-tuning...\nConfiguring model for fine-tuning...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecd33f1c113b4b50af1d66074eddcc4a"}},"metadata":{}},{"name":"stdout","text":"trainable params: 32,788,480 || all params: 4,332,867,952 || trainable%: 0.7567\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/111048 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d42eebe361b4ba0a9fb1f946e9744a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/111048 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59a8c2dbde094ccfb776246a9e16e16f"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"Processed dataset size: 1000 examples\nStarting fine-tuning...\n","output_type":"stream"},{"name":"stderr","text":"It is strongly recommended to train Gemma3 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 6:43:02, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>71.437900</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>58.880800</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>21.993800</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>5.412500</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>3.356400</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.746200</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>2.470900</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>2.203900</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>2.386300</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.205500</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>2.203400</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>2.501700</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.942200</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.962500</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.857700</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.954400</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.834400</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.804100</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>1.940200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.919700</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>1.860400</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.828000</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>1.948400</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>1.845900</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.851000</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>1.297200</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>1.340300</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>1.260000</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>1.377100</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.346800</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>1.330100</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>1.318200</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>1.359700</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>1.323900</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.369400</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>1.321200</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>1.387500</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>1.191100</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.986700</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.964900</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.968700</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.965200</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.992600</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.958700</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.958300</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>1.019900</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.968300</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.966000</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>1.011400</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.970900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning complete! Saving model...\nModel saved to finetuned_model\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"MODEL_NAME = \"google/gemma-2b-it\"\n\nclass YouTubeTranscriptProcessor:\n    \"\"\"\n    A class to process YouTube video transcripts and generate educational content using Gemma-3 model \n    directly from HuggingFace. Optimized for faster generation.\n    \"\"\"\n    \n    def __init__(self, model_name=MODEL_NAME, device=None):\n        \"\"\"\n        Initialize the processor with the specified model.\n        \n        Args:\n            model_name: HuggingFace model name\n            device: The device to run the model on (cuda, cpu, etc.)\n        \"\"\"\n        self.model_name = model_name\n        \n        if device is None:\n            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        else:\n            self.device = device\n            \n        print(f\"Using device: {self.device}\")\n        \n        # Initialize model and tokenizer\n        self._init_model()\n    \n    def _init_model(self):\n        \"\"\"Initialize the model and tokenizer from HuggingFace\"\"\"\n        try:\n            print(f\"Loading model from HuggingFace: {self.model_name}\")\n            print(\"This may take a few moments for the first download...\")\n            \n            # Load tokenizer directly from HuggingFace\n            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n            \n            # Load model directly from HuggingFace with optimizations\n            self.model = AutoModelForCausalLM.from_pretrained(\n                self.model_name,\n                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n                device_map=\"auto\" if self.device == \"cuda\" else None,\n                low_cpu_mem_usage=True\n            )\n            \n            # Move model to appropriate device if not using auto device map\n            if not (self.device == \"cuda\"):\n                self.model = self.model.to(self.device)\n            \n            self.model.eval()  # Set to evaluation mode\n            \n            print(f\"✓ Model loaded successfully\")\n        except Exception as e:\n            print(f\"Error loading model: {e}\")\n            raise\n    \n    def get_youtube_transcript(self, video_id_or_url):\n        \"\"\"Get transcript from a YouTube video.\"\"\"\n        # Extract video ID from URL if needed\n        if \"youtube.com\" in video_id_or_url or \"youtu.be\" in video_id_or_url:\n            video_id = self._extract_video_id(video_id_or_url)\n        else:\n            video_id = video_id_or_url\n            \n        print(f\"Getting transcript for video ID: {video_id}\")\n        \n        try:\n            # Get the transcript\n            transcript_list = YouTubeTranscriptApi.get_transcript(video_id)\n            \n            # Combine all text entries into a single string\n            transcript_text = \" \".join([entry[\"text\"] for entry in transcript_list])\n            \n            # Clean up the transcript text\n            transcript_text = self._clean_transcript(transcript_text)\n            \n            # Truncate long transcripts for faster processing\n            if len(transcript_text) > 3000:\n                print(f\"Transcript too long, truncating to 3000 characters for faster processing\")\n                transcript_text = transcript_text[:3000]\n            \n            print(f\"Successfully retrieved transcript ({len(transcript_text)} characters)\")\n            return transcript_text\n            \n        except Exception as e:\n            print(f\"Error getting transcript: {str(e)}\")\n            raise\n\n    def _extract_video_id(self, url):\n        \"\"\"Extract YouTube video ID from URL.\"\"\"\n        # Handle youtu.be URLs\n        if \"youtu.be\" in url:\n            return url.split(\"/\")[-1].split(\"?\")[0]\n            \n        # Handle youtube.com URLs\n        if \"v=\" in url:\n            return url.split(\"v=\")[1].split(\"&\")[0]\n            \n        # Handle other youtube.com formats\n        if \"youtube.com/embed/\" in url:\n            return url.split(\"/embed/\")[1].split(\"?\")[0]\n            \n        if \"youtube.com/watch/\" in url:\n            return url.split(\"/watch/\")[1].split(\"?\")[0]\n        \n        raise ValueError(f\"Could not extract video ID from URL: {url}\")\n\n    def _clean_transcript(self, transcript_text):\n        \"\"\"Clean up transcript text by removing unnecessary elements.\"\"\"\n        # Remove timestamps if present\n        transcript_text = re.sub(r'\\[\\d+:\\d+:\\d+\\]|\\[\\d+:\\d+\\]', '', transcript_text)\n        \n        # Remove speaker identifications if present (common in auto-generated transcripts)\n        transcript_text = re.sub(r'\\[[^\\]]+\\]:', '', transcript_text)\n        \n        # Remove music indicators\n        transcript_text = re.sub(r'\\[Music\\]|\\[music\\]', '', transcript_text)\n        \n        # Remove extra whitespace and normalize spacing\n        transcript_text = re.sub(r'\\s+', ' ', transcript_text).strip()\n        \n        return transcript_text\n\n    def generate_lecture_content(self, transcript_text):\n        \"\"\"Generate engaging lecture content from the transcript.\"\"\"\n        print(\"Generating lecture content...\")\n        \n        # Create a simpler, shorter prompt for faster generation\n        prompt = f\"\"\"Create a structured educational lecture from this transcript:\n\n{transcript_text[:1000]}\n\nUse markdown headings, bullet points, emojis, and simple language for kids 10-16.\"\"\"\n        \n        # Generate lecture content with reduced tokens\n        return self._generate_text(prompt, max_new_tokens=800)  # Reduced from 2048\n\n    def generate_cheat_sheet(self, transcript_text):\n        \"\"\"Generate a concise cheat sheet from the transcript.\"\"\"\n        print(\"Generating cheat sheet...\")\n        \n        # Create a simpler, shorter prompt for faster generation\n        prompt = f\"\"\"Create a cheat sheet from this transcript:\n\n{transcript_text[:800]}\n\nUse markdown headings, emojis, and key points only.\"\"\"\n        \n        # Generate cheat sheet with reduced tokens\n        return self._generate_text(prompt, max_new_tokens=400)  # Reduced from 1024\n\n    def _generate_text(self, prompt, max_new_tokens=800):\n        \"\"\"Generate text using the model with faster parameters.\"\"\"\n        # Tokenize input\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n        inputs = inputs.to(self.device)\n        input_length = len(inputs[\"input_ids\"][0])\n        \n        print(f\"Input length: {input_length} tokens, generating {max_new_tokens} new tokens\")\n        \n        try:\n            print(\"Generating content...\")\n            start_time = torch.cuda.Event(enable_timing=True) if self.device == \"cuda\" else None\n            end_time = torch.cuda.Event(enable_timing=True) if self.device == \"cuda\" else None\n            \n            if start_time: start_time.record()\n            \n            # Generate with faster parameters\n            with torch.no_grad():\n                outputs = self.model.generate(\n                    **inputs,\n                    max_new_tokens=max_new_tokens,\n                    temperature=0.7,\n                    top_p=0.9,\n                    do_sample=True,\n                    num_return_sequences=1,\n                    pad_token_id=self.tokenizer.pad_token_id or self.tokenizer.eos_token_id,\n                    eos_token_id=self.tokenizer.eos_token_id,\n                    no_repeat_ngram_size=2,  # Prevent repetition\n                    use_cache=True  # Enable caching for faster generation\n                )\n            \n            if end_time:\n                end_time.record()\n                torch.cuda.synchronize()\n                elapsed_time = start_time.elapsed_time(end_time) / 1000  # Convert to seconds\n                print(f\"Generation time: {elapsed_time:.2f} seconds\")\n            \n            # Decode the generated text\n            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n            \n            # Remove prompt from output if it appears at the beginning\n            if generated_text.startswith(prompt):\n                generated_text = generated_text[len(prompt):].strip()\n                \n            print(f\"Generated {len(generated_text)} characters\")\n            \n            return generated_text\n        \n        except Exception as e:\n            print(f\"Error during generation: {e}\")\n            return \"Error generating content\"\n    \n    def process_and_print_video(self, video_id_or_url):\n        \"\"\"Process a YouTube video to generate educational content and print it to console.\"\"\"\n        print(\"\\n\" + \"=\"*50)\n        print(\"📚 EDUCATIONAL CONTENT GENERATOR 📚\")\n        print(\"=\"*50 + \"\\n\")\n        \n        print(f\"Video: {video_id_or_url}\")\n        print(\"-\" * 50)\n        \n        try:\n            # Get the transcript\n            transcript = self.get_youtube_transcript(video_id_or_url)\n            \n            print(\"\\n✓ Transcript successfully retrieved\")\n            print(\"-\" * 50)\n            \n            # Generate lecture content\n            lecture_content = self.generate_lecture_content(transcript)\n            \n            # Generate cheat sheet\n            cheat_sheet = self.generate_cheat_sheet(transcript)\n            \n            # Print results with formatting\n            print(\"\\n\" + \"=\"*50)\n            print(\"📚 EDUCATIONAL LECTURE CONTENT 📚\")\n            print(\"=\"*50 + \"\\n\")\n            print(lecture_content)\n            \n            print(\"\\n\" + \"=\"*50)\n            print(\"📝 QUICK REFERENCE CHEAT SHEET 📝\")\n            print(\"=\"*50 + \"\\n\")\n            print(cheat_sheet)\n            \n            print(\"\\n\" + \"=\"*50)\n            print(\"✅ Processing complete! ✅\")\n            print(\"=\"*50)\n            \n        except Exception as e:\n            print(f\"❌ Error processing video: {e}\")\n            print(\"=\"*50)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T05:59:54.017569Z","iopub.execute_input":"2025-05-04T05:59:54.017881Z","iopub.status.idle":"2025-05-04T05:59:54.040589Z","shell.execute_reply.started":"2025-05-04T05:59:54.017857Z","shell.execute_reply":"2025-05-04T05:59:54.039944Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Main execution function with optimizations\ndef process_youtube_video(video_url, model_name=MODEL_NAME):\n    \"\"\"Main function to process a YouTube video and print educational content.\"\"\"\n    print(\"🚀 Starting YouTube Educational Content Generator 🚀\\n\")\n    \n    # Check GPU availability\n    if torch.cuda.is_available():\n        device = \"cuda\"\n        print(f\"🎮 GPU is available: {torch.cuda.get_device_name(0)}\")\n        print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**2:.0f} MB\")\n    else:\n        device = \"cpu\"\n        print(\"💻 Using CPU (GPU not available)\")\n        print(\"⚠️  CPU processing will be slower. Consider using GPU for faster generation.\")\n    \n    print()\n    \n    try:\n        # Initialize the transcript processor\n        processor = YouTubeTranscriptProcessor(model_name=model_name, device=device)\n        \n        # Process the YouTube video and print results\n        processor.process_and_print_video(video_url)\n        \n    except Exception as main_error:\n        print(f\"❌ Critical error: {main_error}\")\n        print(\"Please check your internet connection and ensure you have the required packages installed.\")\n\n# Run the script\nif __name__ == \"__main__\":\n    # Video URL to process\n    video_url = \"https://www.youtube.com/watch?v=XOfNHCGfJEM&list=PL7wwY6Ln64K4Ev1KQMNwudrh4YqtCTHf3&index=5\"\n    \n    print(\"Welcome to the Educational Content Generator!\")\n    print(\"-\" * 40)\n    print(\"This tool converts YouTube videos into structured\")\n    print(\"educational content with emojis for kids aged 10-16.\")\n    print(\"-\" * 40 + \"\\n\")\n    \n    # Process the video\n    process_youtube_video(video_url)\n    \n    print(\"\\n\" + \"=\" * 50)\n    print(\"Thank you for using Educational Content Generator!\")\n    print(\"=\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T05:59:54.041237Z","iopub.execute_input":"2025-05-04T05:59:54.041408Z","iopub.status.idle":"2025-05-04T06:03:01.865113Z","shell.execute_reply.started":"2025-05-04T05:59:54.041394Z","shell.execute_reply":"2025-05-04T06:03:01.864317Z"}},"outputs":[{"name":"stdout","text":"Welcome to the Educational Content Generator!\n----------------------------------------\nThis tool converts YouTube videos into structured\neducational content with emojis for kids aged 10-16.\n----------------------------------------\n\n🚀 Starting YouTube Educational Content Generator 🚀\n\n🎮 GPU is available: Tesla T4\n   GPU Memory: 15095 MB\n\nUsing device: cuda\nLoading model from HuggingFace: google/gemma-2b-it\nThis may take a few moments for the first download...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f9e513eb6f64a819b892f5a1523b2e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13098b9a7b0b44cba68ac7704c7641cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d96a2be362704cd8934a3479fa84c509"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fcedfdbf73e40dd99e4ba05e97f5c7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79e4a7208aac42cc9774798da0b29323"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4e31ac7f177479eba085af8eb445593"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"263f95201718496c972f7136191e207f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ea1cc77944547b7901bac1a2a4fdcb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a9559a56b004d21ae062fe0ec682a7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89d11f225b8c442abc740541d601915f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd1b8b5f731348ef926eb14e5c3f2eec"}},"metadata":{}},{"name":"stdout","text":"✓ Model loaded successfully\n\n==================================================\n📚 EDUCATIONAL CONTENT GENERATOR 📚\n==================================================\n\nVideo: https://www.youtube.com/watch?v=XOfNHCGfJEM&list=PL7wwY6Ln64K4Ev1KQMNwudrh4YqtCTHf3&index=5\n--------------------------------------------------\nGetting transcript for video ID: XOfNHCGfJEM\nTranscript too long, truncating to 3000 characters for faster processing\nSuccessfully retrieved transcript (3000 characters)\n\n✓ Transcript successfully retrieved\n--------------------------------------------------\nGenerating lecture content...\nInput length: 250 tokens, generating 800 new tokens\nGenerating content...\nGeneration time: 19.56 seconds\nGenerated 1845 characters\nGenerating cheat sheet...\nInput length: 199 tokens, generating 400 new tokens\nGenerating content...\nGeneration time: 3.68 seconds\nGenerated 485 characters\n\n==================================================\n📚 EDUCATIONAL LECTURE CONTENT 📚\n==================================================\n\n## Chapter 2: Drawing with Turtles\n\n### Introduction\n\nHey there, welcome back to my channel! Today' s lesson is all about drawing with Python using our favorite furry friend, the **turtle**!\n\n**Let' s get started!**\n\n#### 🎨 Creating a Canvas\n\n- First things first, we' need a place to put our turtle drawing.\n- We do that by creating a **canvas** using **pen**.\n```python\nimport turtle\n\n# Create a new turtle object\nturtle_object = turtle.Turtle()\n\nturtle.pensize(3) # Set the line thickness to 3\ncolor = \"green\" # Choose the drawing color\nstart_x = 50\nend_y =155\n# Draw a line from x=51 to x =59 and y =0 to y=170\n\n turtle_ object.forward(end-x)\n turtle _object.left(90) \n turtles_  object .forward (x-53 )\n\n```\n\n##### 🧠 Turtle Basics\n\nLet’s learn the essential commands to make our turtles! 🐢\n\n* **`turtle`**: This is the module containing all our drawing tools. \n\n  * `turtle object = Turtle()`: We create an instance of the `Turtle` class from the Turtle module.\n\n\n*  **`pen`:** This method allows us to set the thickness of our lines. (3 in this case)\n\n\n   *` turtle .pensize( 4)`: This sets the width of each line to be  0.04 units.\n\n\n\n*   **Color**: It sets our pen color. (\"Green\" in our case).\n\n\n  \n*     **Start and end coordinates**: These define the starting and ending points of where the lines will be drawn. In this example, it starts at x- 60, y-0 and ends at  x=69,y-200\n\n\n**Additional Commands:**\n\n - `forward` moves the cursor along the canvas.  \n\n\n- `left` rotates the head of  the turtle by 9 degrees. This makes it look left.\n\n\n\n\n### Conclusion\n\nThat' a basic introduction to drawing using turtles in Python! We've covered setting up a drawing canvas, understanding the basic turtle commands,  and drawing a simple line. Now, let’ s explore some more advanced techniques and create some amazing turtle drawings of your own!\n\n==================================================\n📝 QUICK REFERENCE CHEAT SHEET 📝\n==================================================\n\n# Chapter 2: Introduction to Turtle Drawing\n\n**Key Points:**\n\n- The turtle package provides the necessary commands for creating and manipulating turtles.\n- Turtles can be drawn with various shapes and colors.\n\n\n# **Import the Turtle Module**\n\n```python\nimport turtle\n```\n\n---\n\nThis is just a starting point for learning about turtle drawing in Python. There is much more to explore and learn, but this should give you a basic understanding of how to import and use the `turtle` module.\n\n==================================================\n✅ Processing complete! ✅\n==================================================\n\n==================================================\nThank you for using Educational Content Generator!\n==================================================\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Interactive demo - run this cell to process a video URL provided by you\nif __name__ == \"__main__\":\n    video_url = input(\"Enter YouTube video URL: \")\n    \n    results = process_youtube_video(\n        video_url=video_url,\n        model_name=MODEL_NAME,  # Will use fine-tuned model if available, otherwise original HF model\n        output_dir=OUTPUTS_DIR\n    )\n    \n    print(\"\\nProcessing complete!\")\n    print(f\"Lecture PDF: {results['lecture_pdf_path']}\")\n    print(f\"Cheat Sheet PDF: {results['cheat_sheet_pdf_path']}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_model_for_download(model_path, output_filename=None):\n    \"\"\"\n    Prepare the model for download by creating a ZIP archive.\n    \n    Args:\n        model_path: Path to the model directory\n        output_filename: Name of the output ZIP file (optional)\n        \n    Returns:\n        Path to the ZIP file\n    \"\"\"\n    import os\n    import zipfile\n    import time\n    \n    if output_filename is None:\n        output_filename = f\"model_export_{int(time.time())}.zip\"\n    \n    # Make sure the output path is absolute\n    if not os.path.isabs(output_filename):\n        output_filename = os.path.join(os.getcwd(), output_filename)\n    \n    try:\n        print(f\"Creating ZIP archive of model at {model_path}...\")\n        \n        # Create a ZIP file\n        with zipfile.ZipFile(output_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n            # Walk through all files in the directory\n            for root, dirs, files in os.walk(model_path):\n                for file in files:\n                    # Create the full file path\n                    file_path = os.path.join(root, file)\n                    # Calculate the relative path for the archive\n                    relative_path = os.path.relpath(file_path, os.path.dirname(model_path))\n                    # Add the file to the ZIP\n                    zipf.write(file_path, relative_path)\n        \n        print(f\"Model archive created successfully at: {output_filename}\")\n        return output_filename\n    \n    except Exception as e:\n        print(f\"Error creating model archive: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T12:46:56.591060Z","iopub.execute_input":"2025-05-03T12:46:56.591673Z","iopub.status.idle":"2025-05-03T12:46:56.600747Z","shell.execute_reply.started":"2025-05-03T12:46:56.591648Z","shell.execute_reply":"2025-05-03T12:46:56.600135Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Define the path to your model\nmodel_path = MODEL_NAME  # or the path where you saved your model\n\n# Create a ZIP archive\nzip_path = prepare_model_for_download(model_path, \"my_finetuned_model.zip\")\n\n# For Kaggle environments\nif 'google.colab' in str(get_ipython()):\n    from google.colab import files\n    if zip_path and os.path.exists(zip_path):\n        print(\"Initiating download...\")\n        files.download(zip_path)\n    else:\n        print(\"ZIP file creation failed or file does not exist.\")\n# For Kaggle environments\nelif 'kaggle' in str(get_ipython()):\n    print(f\"Your model has been zipped to: {zip_path}\")\n    print(\"You can now download it from the Kaggle interface by clicking on the file in the output section.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T12:47:01.857587Z","iopub.execute_input":"2025-05-03T12:47:01.858260Z","iopub.status.idle":"2025-05-03T12:47:21.235035Z","shell.execute_reply.started":"2025-05-03T12:47:01.858237Z","shell.execute_reply":"2025-05-03T12:47:21.234366Z"}},"outputs":[{"name":"stdout","text":"Creating ZIP archive of model at /kaggle/working/finetuned_model/checkpoint-500...\nModel archive created successfully at: /kaggle/working/my_finetuned_model.zip\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
