{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Introduction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:45:59.005061Z",
     "iopub.status.busy": "2025-04-01T14:45:59.004740Z",
     "iopub.status.idle": "2025-04-01T14:47:11.038548Z",
     "shell.execute_reply": "2025-04-01T14:47:11.037398Z",
     "shell.execute_reply.started": "2025-04-01T14:45:59.005034Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -q bitsandbytes\n",
    "%pip install -q transformers\n",
    "%pip install -q peft\n",
    "%pip install -q accelerate\n",
    "%pip install -q trl\n",
    "%pip install -q torch\n",
    "%pip install -q langchain pypdf sentence-transformers\n",
    "%pip install -U langchain-community\n",
    "%pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load all libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:47:11.040223Z",
     "iopub.status.busy": "2025-04-01T14:47:11.039968Z",
     "iopub.status.idle": "2025-04-01T14:47:37.789145Z",
     "shell.execute_reply": "2025-04-01T14:47:37.788272Z",
     "shell.execute_reply.started": "2025-04-01T14:47:11.040202Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig, TrainingArguments, pipeline\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "from IPython.display import Markdown, display\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code below configures a large language model (LLM) for inference with quantization techniques for efficiency. Here's a breakdown of what each part does:\n",
    "\n",
    "**Model Path and Quantization Configuration**\n",
    "\n",
    "1. **Model Path:** The `model` variable stores the path to a pre-trained causal language model (likely a 2-billion parameter model) on Kaggle Datasets.\n",
    "\n",
    "2. **BitsAndBytesConfig:** The `bnbConfig` object defines the configuration for quantization using the BitsAndBytes library. Here are the key arguments:\n",
    "    * `load_in_4bit (bool, optional)`: This argument enables 4-bit quantization, reducing memory usage by approximately fourfold compared to the original model.\n",
    "    * `bnb_4bit_quant_type (str, optional)`: This parameter specifies the type of 4-bit quantization to use. Here, it's set to `\"nf4\"`, a specific quantization format supported by BitsAndBytes.\n",
    "    * `bnb_4bit_compute_dtype (torch.dtype, optional)`: This argument defines the data type used for computations during inference. Here, it's set to `torch.bfloat16`, a lower-precision format that can improve speed on compatible hardware.\n",
    "\n",
    "**Loading Tokenizer and Model with Quantization**\n",
    "\n",
    "1. **AutoTokenizer:** The `AutoTokenizer.from_pretrained` function loads the tokenizer associated with the pre-trained model at the specified path (`model`). The `quantization_config` argument is crucial here. It tells the tokenizer to consider the quantization information (e.g., potential padding changes) while processing text.\n",
    "\n",
    "2. **AutoModelForCausalLM:** Similarly, `AutoModelForCausalLM.from_pretrained` loads the actual LLM model from the path (`model`). Again, the `device_map=\"auto\"` argument allows automatic device placement (CPU or GPU) and the `quantization_config` ensures the model is loaded with the 4-bit quantization configuration.\n",
    "\n",
    "**Overall, this code snippet aims to achieve two goals:**\n",
    "\n",
    "* **Load a pre-trained LLM:** It retrieves a pre-trained causal language model from the specified path.\n",
    "* **Enable Quantization for Efficiency:** By using the `BitsAndBytesConfig` and arguments during loading, the code configures the tokenizer and model to leverage 4-bit quantization for memory reduction and potentially faster inference on compatible hardware.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><strong>Know More about <a href=\"https://www.kaggle.com/code/lorentzyeung/what-s-4-bit-quantization-how-does-it-help-llama2\">4-bit quantization</a></strong></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:47:37.790913Z",
     "iopub.status.busy": "2025-04-01T14:47:37.790631Z",
     "iopub.status.idle": "2025-04-01T14:48:50.183795Z",
     "shell.execute_reply": "2025-04-01T14:48:50.182912Z",
     "shell.execute_reply.started": "2025-04-01T14:47:37.790893Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d113e7ba6ff4f6cad90fb30ee6447cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1742e871f4504fe080f43393e68db525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d435ad49b17e4999ac81db079b6b1692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "016a4a447e2b4367833522bd5c1da860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "407d5b2fcebe4e1e94ed9381a4c5fff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/23.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc66682af21647dbbed437cb93fd14b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93782e5cc6644335b6383b7251bd705a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "098317bf8fc6486991a9adb5e8324496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2190b70bcbbd4e508f38b5b722c617c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma-2b-it.gguf:   0%|          | 0.00/10.0G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f658c30e9c1543f7a3ba611aaf77e56e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0901e502e4a745368a61d4515c2242f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "188500a8800741fda862dec3c8c7d619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8531520c5ce34f67b732edd5e6b3142c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download , login\n",
    "\n",
    "\n",
    "login(token=\"hf_\") \n",
    "\n",
    "model_path = snapshot_download(\"google/gemma-2b-it\")\n",
    "\n",
    "print(model_path)  # Check where it was downloaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:48:50.185600Z",
     "iopub.status.busy": "2025-04-01T14:48:50.185325Z",
     "iopub.status.idle": "2025-04-01T14:48:50.188998Z",
     "shell.execute_reply": "2025-04-01T14:48:50.188313Z",
     "shell.execute_reply.started": "2025-04-01T14:48:50.185576Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# # Save the model (already done)\n",
    "# model.save_pretrained(\"./fine_tuned_gemma\")\n",
    "\n",
    "# # Save the tokenizer with a cleaned config\n",
    "# save_directory = \"./fine_tuned_gemma\"\n",
    "\n",
    "# # Extract essential tokenizer config\n",
    "# tokenizer_config = {\n",
    "#     \"tokenizer_class\": tokenizer.__class__.__name__,\n",
    "#     \"model_max_length\": tokenizer.model_max_length,\n",
    "#     \"padding_side\": tokenizer.padding_side,\n",
    "#     \"truncation_side\": tokenizer.truncation_side,\n",
    "#     \"bos_token\": tokenizer.bos_token,\n",
    "#     \"eos_token\": tokenizer.eos_token,\n",
    "#     \"unk_token\": tokenizer.unk_token,\n",
    "#     \"pad_token\": tokenizer.pad_token,\n",
    "#     \"additional_special_tokens\": tokenizer.additional_special_tokens,\n",
    "#     \"chat_template\": tokenizer.chat_template,\n",
    "#     \"name_or_path\": tokenizer.name_or_path,\n",
    "#     \"add_bos_token\": tokenizer._add_bos_token,\n",
    "#     \"add_eos_token\": tokenizer._add_eos_token,\n",
    "# }\n",
    "\n",
    "# # Save the cleaned config\n",
    "# with open(f\"{save_directory}/tokenizer_config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(tokenizer_config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# # Save the vocabulary and other necessary files\n",
    "# tokenizer.save_vocabulary(save_directory)\n",
    "# # If your tokenizer uses a .model file (e.g., tokenizer.model), copy it\n",
    "# import shutil\n",
    "# vocab_file = tokenizer.init_kwargs.get(\"vocab_file\")\n",
    "# if vocab_file:\n",
    "#     shutil.copy(vocab_file, f\"{save_directory}/tokenizer.model\")\n",
    "\n",
    "# print(\"Tokenizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:48:50.190120Z",
     "iopub.status.busy": "2025-04-01T14:48:50.189928Z",
     "iopub.status.idle": "2025-04-01T14:49:02.511058Z",
     "shell.execute_reply": "2025-04-01T14:49:02.510398Z",
     "shell.execute_reply.started": "2025-04-01T14:48:50.190103Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe136cd0164e4df5bd0ffa4d9e0cc4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model = \"/kaggle/input/gemma/transformers/2b-it/2\"\n",
    "\n",
    "bnbConfig = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, device_map=\"auto\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map = \"auto\",\n",
    "    quantization_config=bnbConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Fine Tune Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:49:02.512390Z",
     "iopub.status.busy": "2025-04-01T14:49:02.512039Z",
     "iopub.status.idle": "2025-04-01T14:49:02.640847Z",
     "shell.execute_reply": "2025-04-01T14:49:02.639677Z",
     "shell.execute_reply.started": "2025-04-01T14:49:02.512358Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Child_Friendly_Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the difference between a variable and...</td>\n",
       "      <td>[\"Sure, here's the difference between a variab...</td>\n",
       "      <td>, here's the difference between a a magical tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the difference between a built-in fun...</td>\n",
       "      <td>[\"Sure. Here's the difference between built-in...</td>\n",
       "      <td>. here's the difference between built-in a sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the difference between the `print` fu...</td>\n",
       "      <td>[\"Sure. Here's the difference between the two ...</td>\n",
       "      <td>. here's the difference between the two a supe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the difference between an expression ...</td>\n",
       "      <td>[\"Sure! Here's the difference between an expre...</td>\n",
       "      <td>! here's the difference between an a tiny reci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the difference between `True` and `Fa...</td>\n",
       "      <td>[\"Sure. Here's the difference between `True` a...</td>\n",
       "      <td>. here's the difference between `true` and `fa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0   What is the difference between a variable and...   \n",
       "1   What is the difference between a built-in fun...   \n",
       "2   What is the difference between the `print` fu...   \n",
       "3   What is the difference between an expression ...   \n",
       "4   What is the difference between `True` and `Fa...   \n",
       "\n",
       "                                              Answer  \\\n",
       "0  [\"Sure, here's the difference between a variab...   \n",
       "1  [\"Sure. Here's the difference between built-in...   \n",
       "2  [\"Sure. Here's the difference between the two ...   \n",
       "3  [\"Sure! Here's the difference between an expre...   \n",
       "4  [\"Sure. Here's the difference between `True` a...   \n",
       "\n",
       "                               Child_Friendly_Answer  \n",
       "0  , here's the difference between a a magical tr...  \n",
       "1  . here's the difference between built-in a sup...  \n",
       "2  . here's the difference between the two a supe...  \n",
       "3  ! here's the difference between an a tiny reci...  \n",
       "4  . here's the difference between `true` and `fa...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = pd.read_csv(\"/kaggle/input/dataset-python-question-answer/Dataset_Python_Question_Answer.csv\")\n",
    "# dataset = Dataset.from_pandas(data)\n",
    "# data.head()\n",
    "data = pd.read_csv(\"/kaggle/input/child-friendly-dataset/Child_Friendly_Python_QA.csv\")\n",
    "# data = data.drop(columns=[\"Answer\"], errors='ignore')  # 'ignore' prevents errors if column doesn't exist\n",
    "# data = data.rename(columns={\"Child_Friendly_Answer\": \"Answer\"})\n",
    "dataset = Dataset.from_pandas(data)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define a formatting function for the model output.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:49:02.642323Z",
     "iopub.status.busy": "2025-04-01T14:49:02.641972Z",
     "iopub.status.idle": "2025-04-01T14:49:02.647028Z",
     "shell.execute_reply": "2025-04-01T14:49:02.646039Z",
     "shell.execute_reply.started": "2025-04-01T14:49:02.642283Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Question', 'Answer', 'Child_Friendly_Answer']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:49:02.648282Z",
     "iopub.status.busy": "2025-04-01T14:49:02.647975Z",
     "iopub.status.idle": "2025-04-01T14:49:02.661044Z",
     "shell.execute_reply": "2025-04-01T14:49:02.660178Z",
     "shell.execute_reply.started": "2025-04-01T14:49:02.648251Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
    "    line = template.format(instruction=example['Question'], response=example['Answer'])\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:49:02.663519Z",
     "iopub.status.busy": "2025-04-01T14:49:02.663292Z",
     "iopub.status.idle": "2025-04-01T14:49:02.674031Z",
     "shell.execute_reply": "2025-04-01T14:49:02.673250Z",
     "shell.execute_reply.started": "2025-04-01T14:49:02.663500Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **What is WANDB?**\n",
    "\n",
    "WANDB is a cloud platform for experiment tracking specifically designed for machine learning. It provides functionalities like:\n",
    "\n",
    "* **Logging** training metrics, parameters, and visualizations.\n",
    "* **Version control** for your machine learning experiments.\n",
    "* **Sweeping** hyperparameters to find the best performing configuration.\n",
    "* **Collaboration** features to share and discuss experiments with your team.\n",
    "\n",
    "## **Why disable WANDB?**\n",
    "\n",
    "There are a few reasons why you might want to disable WANDB:\n",
    "\n",
    "* **You don't need experiment tracking:** If your code is a simple experiment or you're not interested in tracking the results, disabling WANDB can reduce overhead.\n",
    "* **Privacy concerns:** WANDB logs your experiment data to the cloud. If your data is sensitive, you might not want to upload it.\n",
    "* **Troubleshooting errors:**  Sometimes errors can arise from WANDB itself. Disabling it can help isolate the issue.\n",
    "\n",
    "## **Setting the `WANDB_DISABLED` environment variable**\n",
    "\n",
    "We sets an environment variable called `WANDB_DISABLED` to `\"true\"`. This tells the WANDB library to not initialize itself, effectively disabling it for current script.\n",
    "\n",
    "## **In summary:**\n",
    "\n",
    "* WANDB is a useful tool for experiment tracking in machine learning.\n",
    "* You might disable WANDB if you don't need experiment tracking or for debugging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:49:02.675890Z",
     "iopub.status.busy": "2025-04-01T14:49:02.675682Z",
     "iopub.status.idle": "2025-04-01T14:49:02.687006Z",
     "shell.execute_reply": "2025-04-01T14:49:02.686205Z",
     "shell.execute_reply.started": "2025-04-01T14:49:02.675873Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r = 8,\n",
    "    target_modules = [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type = \"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code above defines a configuration object called `lora_config` for a technique called LoRA (Low-Rank Adaptation). Here's a breakdown of what each parameter does:\n",
    "\n",
    "**LoRA - Low-Rank Adaptation**\n",
    "\n",
    "LoRA is a technique used to fine-tune large language models (LLMs) more efficiently. It allows you to adapt pre-trained models to new tasks with minimal memory and computational cost compared to traditional fine-tuning.\n",
    "\n",
    "**LoraConfig Parameters:**\n",
    "\n",
    "* **r (int):** This parameter defines the rank of the low-rank decomposition used in LoRA. It controls the trade-off between accuracy and memory usage. A lower value of `r` uses less memory but might lead to slightly lower accuracy. The default value is typically 8, as set in our code.\n",
    "\n",
    "* **target_modules (List[str]):** This list specifies the Transformer layers where LoRA will be applied. The provided configuration targets several key projection layers within the Transformer architecture:\n",
    "    * `q_proj`: Query projection\n",
    "    * `o_proj`: Output projection\n",
    "    * `k_proj`: Key projection\n",
    "    * `v_proj`: Value projection\n",
    "    * `gate_proj`: Gate projection (used in attention layers)\n",
    "    * `up_proj`: Upsampling projection (used in some encoder-decoder architectures)\n",
    "    * `down_proj`: Downsampling projection (used in some encoder-decoder architectures)\n",
    "\n",
    "By applying LoRA to these projection layers, the model can learn task-specific adaptations without modifying the original large model weights significantly.\n",
    "\n",
    "* **task_type (str, optional):** This parameter specifies the type of task you're fine-tuning the model for. While not used in this specific configuration, some libraries might leverage this information to optimize LoRA for specific task categories (e.g., \"CAUSAL_LM\" for causal language modeling).\n",
    "\n",
    "**In summary:**\n",
    "\n",
    "This configuration defines how LoRA will be applied to a pre-trained model for fine-tuning. It specifies the rank of the decomposition (memory usage) and the target layers within the Transformer architecture where LoRA will be used to adapt the model to a new task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:49:02.688123Z",
     "iopub.status.busy": "2025-04-01T14:49:02.687890Z",
     "iopub.status.idle": "2025-04-01T14:49:02.700953Z",
     "shell.execute_reply": "2025-04-01T14:49:02.700112Z",
     "shell.execute_reply.started": "2025-04-01T14:49:02.688104Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# tokenizer.model_max_length = 512 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:49:02.702336Z",
     "iopub.status.busy": "2025-04-01T14:49:02.702041Z",
     "iopub.status.idle": "2025-04-01T14:49:05.926599Z",
     "shell.execute_reply": "2025-04-01T14:49:05.925938Z",
     "shell.execute_reply.started": "2025-04-01T14:49:02.702308Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dcbaef6ec124ebaad0ea0bcc208e84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/419 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f1f2b5a9514f43919648d97412679d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/419 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62e75ba35e64ff4808ce424e6d62476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/419 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19be5900e78466ea01a7e81e645e18e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/419 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c9f967604246bab543c5aa92a9c637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/419 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    \n",
    "    model=model,\n",
    "    train_dataset=dataset\n",
    "    \n",
    "    ,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=2,\n",
    "        max_steps=50,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        # dataset_text_field=\"text\" \n",
    "        # max_seq_length=512,\n",
    "    )\n",
    "    \n",
    "    ,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    "    # dataset_kwargs={\"batched\": False} \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code above creates an instance of `SFTTrainer` from the Transformers library specifically designed for supervised fine-tuning (SFT) tasks. Here's a breakdown of what each part does:\n",
    "\n",
    "**SFTTrainer for Supervised Fine-Tuning**\n",
    "\n",
    "This code utilizes `SFTTrainer` to fine-tune a pre-trained model (`model`) on a specific training dataset (`dataset`). It's designed for tasks where you have labeled data and want to adapt the model for a new purpose.\n",
    "\n",
    "**Key Parameters:**\n",
    "\n",
    "* **model (PreTrainedModel):** This argument specifies the pre-trained model you want to fine-tune.\n",
    "\n",
    "* **train_dataset (Dataset):** This argument points to the training dataset you'll use for fine-tuning. The dataset should be formatted appropriately for the task.\n",
    "\n",
    "* **max_seq_length (int):** This parameter defines the maximum sequence length allowed in the training data. Sequences exceeding this length will be truncated.\n",
    "\n",
    "* **args (TrainingArguments):** This argument is an instance of `TrainingArguments` that defines various hyperparameters for the training process. Here are some notable arguments within `args`:\n",
    "    * `per_device_train_batch_size (int)`: Sets the batch size per device (GPU/TPU) during training. Here, it's set to 1, which is a small batch size commonly used with gradient accumulation.\n",
    "    * `gradient_accumulation_steps (int)`: This parameter allows accumulating gradients over several batches before updating the model weights. Here, it's set to 4, effectively increasing the effective batch size.\n",
    "    * `warmup_steps (int)`: This defines the number of warmup steps where the learning rate is gradually increased from 0 to its full value. Here, it's set to 2.\n",
    "    * `max_steps (int)`: This parameter specifies the total number of training steps. Here, it's set to 50, which might be a short training run for fine-tuning depending on your dataset size and task complexity.\n",
    "    * `learning_rate (float)`: This sets the learning rate for the optimizer. Here, it's set to 2e-4, which is a common starting point for fine-tuning.\n",
    "    * `fp16 (bool)`: Enables training using 16-bit floating-point precision (mixed precision) for faster training with minimal accuracy loss (if supported by your hardware).\n",
    "    * `logging_steps (int)`: Defines how often training metrics are logged during training. Here, it's set to 1, logging metrics for every step. \n",
    "    * `output_dir (str)`: Specifies the directory where training outputs (model checkpoints, logs, etc.) will be saved. Here, it's set to \"outputs\".\n",
    "    * `optim (str)`: Defines the optimizer used for training. Here, it's set to \"paged_adamw_8bit\", which is likely an optimizer with specific memory optimizations. \n",
    "\n",
    "* **peft_config (LoraConfig):** This argument is likely referencing the `lora_config` you defined earlier. It provides the configuration for LoRA (Low-Rank Adaptation), which helps fine-tune the model more efficiently.\n",
    "\n",
    "* **formatting_func (Callable):** This argument (if provided) specifies a custom function for formatting the training data before feeding it to the model. This allows for specific pre-processing steps tailored to your task.\n",
    "\n",
    "**In essence:**\n",
    "\n",
    "This code snippet configures and initializes an `SFTTrainer` for fine-tuning a pre-trained model with LoRA for memory efficiency. The training hyperparameters are set within the `TrainingArguments` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:49:05.927595Z",
     "iopub.status.busy": "2025-04-01T14:49:05.927374Z",
     "iopub.status.idle": "2025-04-01T14:51:03.425069Z",
     "shell.execute_reply": "2025-04-01T14:51:03.424386Z",
     "shell.execute_reply.started": "2025-04-01T14:49:05.927577Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 01:53, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.604800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.503600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.879700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.264300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.899900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>5.664400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.209700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.791700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.518700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.510100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.216500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.552600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.385800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.928400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.895300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.566400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.396800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.806600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.641700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.668300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.426600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.697700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.958200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.374600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.253100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.396500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.505900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.097100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.038400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.445900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.571200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.315200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.807600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.025200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.310200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.136600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.406700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.439700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.747100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>2.181100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>2.054000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.893500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.920600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.298900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.693200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.993300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>2.302300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.663500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.541700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=2.8512011623382567, metrics={'train_runtime': 117.0933, 'train_samples_per_second': 1.708, 'train_steps_per_second': 0.427, 'total_flos': 967136896880640.0, 'train_loss': 2.8512011623382567})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Retrieval Augment Generation (RAG)**\n",
    "```Retrieval Augmented Generation (RAG)``` is a paradigm in language model architecture that integrates both retrieval and generation processes to enhance the model's understanding and response capabilities. In essence, it combines the strengths of retrieval-based models, which excel at accessing and utilizing external knowledge sources, with generative models, which can generate novel and contextually relevant responses.\n",
    "\n",
    "The primary benefit of RAG in large language models (LLMs) is its ability to leverage external knowledge sources during the generation process. By retrieving relevant information from a predefined knowledge base or corpus, the model can augment its understanding of the input context and produce more accurate and informative responses. This approach not only improves the coherence and relevance of generated text but also enables the model to incorporate real-world knowledge and factual accuracy into its outputs.\n",
    "\n",
    "RAG aims to achieve several key objectives:\n",
    "\n",
    "1. **Enhanced Contextual Understanding:** By retrieving relevant information from external sources, RAG can better understand the context of a given prompt or query, leading to more contextually appropriate responses.\n",
    "\n",
    "2. **Improved Content Quality:** Integrating external knowledge sources allows RAG to generate content that is more accurate, informative, and relevant to the input context, enhancing the overall quality of generated text.\n",
    "\n",
    "3. **Factually Accurate Responses:** By accessing external knowledge bases, RAG can ensure that its responses are factually accurate and grounded in real-world information, reducing the likelihood of generating misleading or incorrect information.\n",
    "\n",
    "The workflow of RAG typically involves the following steps:\n",
    "\n",
    "1. **Retrieval:** The model first retrieves relevant information from a knowledge base or corpus based on the input prompt or query. This retrieval process aims to identify key facts, concepts, or contextually relevant information to inform the generation process.\n",
    "\n",
    "2. **Augmentation:** The retrieved information is then used to augment the model's understanding of the input context. By incorporating this external knowledge, the model can generate more informed and contextually appropriate responses.\n",
    "\n",
    "3. **Generation:** Finally, the model generates a response based on the augmented understanding of the input context, leveraging both the original prompt and the retrieved information to produce a coherent and relevant output.\n",
    "\n",
    "The necessity of using RAG lies in its ability to address the limitations of traditional generative models, such as lack of factual accuracy and coherence in responses. By integrating retrieval-based mechanisms, RAG can access external knowledge sources to enhance its understanding of the input context, leading to more accurate, informative, and contextually relevant generated text. This approach is particularly valuable in tasks requiring a deep understanding of complex topics or access to large knowledge bases, such as question answering, dialogue generation, and content summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load documents for RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:51:03.426114Z",
     "iopub.status.busy": "2025-04-01T14:51:03.425860Z",
     "iopub.status.idle": "2025-04-01T14:52:00.091096Z",
     "shell.execute_reply": "2025-04-01T14:52:00.089929Z",
     "shell.execute_reply.started": "2025-04-01T14:51:03.426087Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Instantiate a PyPDFDirectoryLoader object with the specified directory path\n",
    "pdf_loader = PyPDFDirectoryLoader(\"/kaggle/input/knowledge-base\")\n",
    "\n",
    "# # Load PDF documents from the specified directory\n",
    "pdfs = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:52:00.093091Z",
     "iopub.status.busy": "2025-04-01T14:52:00.092112Z",
     "iopub.status.idle": "2025-04-01T14:52:04.103635Z",
     "shell.execute_reply": "2025-04-01T14:52:04.102880Z",
     "shell.execute_reply.started": "2025-04-01T14:52:00.093057Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-54edb4065953>:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d34b952003e44448b07874cacb4412c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "477d5e1237eb4adbaca28d182993749c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae0ce0d099c4b8c9adda4e60d650b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a4c83aca2141aab95e5e5c8ba94a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c98c6bad24d9423db52526426d6cad59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c7b0e820be344189894ddca675086b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb89af02814a450cba76d45f3b271094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c37b0d1c7f7488bac09db6b8ab7fcbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d09d57930c406e9e5125f872847c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5049c5112d749a5b0681190a5c6dcb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb2a884e5c648269c5fee7719b87fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import the HuggingFaceEmbeddings class, \n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    # This argument specifies the pre-trained model name to be used for generating embeddings.\n",
    "    # Here, \"sentence-transformers/all-mpnet-base-v2\" is a pre-trained sentence transformer model \n",
    "    # from the Sentence Transformers library (not Transformers).\n",
    "    # Sentence transformer models are specifically trained to generate meaningful representations \n",
    "    # of sentences that capture semantic similarity.\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "\n",
    "    # This argument is likely specific to the HuggingFaceEmbeddings class and might \n",
    "    # not be present in the base Transformers library.\n",
    "    # It sets the device to \"cuda\" to leverage the GPU for faster processing if available.\n",
    "    model_kwargs={\"device\": \"cuda\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:52:04.104819Z",
     "iopub.status.busy": "2025-04-01T14:52:04.104505Z",
     "iopub.status.idle": "2025-04-01T14:52:04.625462Z",
     "shell.execute_reply": "2025-04-01T14:52:04.624797Z",
     "shell.execute_reply.started": "2025-04-01T14:52:04.104789Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Instantiate a RecursiveCharacterTextSplitter object with specified parameters\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "# Split documents into chunks using the RecursiveCharacterTextSplitter\n",
    "all_splits = text_splitter.split_documents(pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:52:04.626549Z",
     "iopub.status.busy": "2025-04-01T14:52:04.626265Z",
     "iopub.status.idle": "2025-04-01T14:52:04.629876Z",
     "shell.execute_reply": "2025-04-01T14:52:04.629075Z",
     "shell.execute_reply.started": "2025-04-01T14:52:04.626527Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# zip_file_path = '/content/vectordb.zip'  # Replace with ZIP file's path\n",
    "# extract_dir = '/content/vectordb' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:52:04.631089Z",
     "iopub.status.busy": "2025-04-01T14:52:04.630793Z",
     "iopub.status.idle": "2025-04-01T14:52:04.643578Z",
     "shell.execute_reply": "2025-04-01T14:52:04.642837Z",
     "shell.execute_reply.started": "2025-04-01T14:52:04.631058Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# zip_file_path=\n",
    "# with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(extract_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:52:04.644598Z",
     "iopub.status.busy": "2025-04-01T14:52:04.644301Z",
     "iopub.status.idle": "2025-04-01T14:57:26.025172Z",
     "shell.execute_reply": "2025-04-01T14:57:26.024138Z",
     "shell.execute_reply.started": "2025-04-01T14:52:04.644569Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-547d3bb033e7>:4: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory='./vectordb')\n",
    "#When running in notebook, need to use .persist()\n",
    "vectordb.persist() \n",
    "\n",
    "#load chroma db instead of constructing it again (faster)\n",
    "# vectordb = Chroma(persist_directory=\"/kaggle/input/vectordb2\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:57:26.026649Z",
     "iopub.status.busy": "2025-04-01T14:57:26.026307Z",
     "iopub.status.idle": "2025-04-01T14:57:26.030019Z",
     "shell.execute_reply": "2025-04-01T14:57:26.029303Z",
     "shell.execute_reply.started": "2025-04-01T14:57:26.026616Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create a retriever\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:57:26.031098Z",
     "iopub.status.busy": "2025-04-01T14:57:26.030882Z",
     "iopub.status.idle": "2025-04-01T14:57:26.042994Z",
     "shell.execute_reply": "2025-04-01T14:57:26.042224Z",
     "shell.execute_reply.started": "2025-04-01T14:57:26.031079Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !zip -r vectordb.zip /kaggle/working/vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:57:26.044038Z",
     "iopub.status.busy": "2025-04-01T14:57:26.043783Z",
     "iopub.status.idle": "2025-04-01T14:57:26.054652Z",
     "shell.execute_reply": "2025-04-01T14:57:26.053975Z",
     "shell.execute_reply.started": "2025-04-01T14:57:26.044018Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This code creates a pipeline for text generation using a pre-trained model (model) \n",
    "# and its tokenizer (tokenizer). It leverages mixed precision (torch.bfloat16) \n",
    "# for potentially faster inference and limits generated text to 512 tokens.\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    model_kwargs = {\"torch.dtype\": torch.bfloat16},\n",
    "    max_new_tokens=512  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here's a breakdown of the code snippet below and the parameters used for text generation:\n",
    "\n",
    "**1. Prompt Creation (`pipeline.tokenizer.apply_chat_template`)**\n",
    "\n",
    "* **Function:** This part uses a function (likely) provided by the text-generation pipeline to specifically format the conversation history (`messages`) into a prompt suitable for the LLM. \n",
    "* **Arguments:**\n",
    "    * `messages (List[Dict])`: This argument is a list of dictionaries representing the conversation history. Each dictionary likely contains keys like \"role\" (indicating user or system) and \"content\" (the actual text).\n",
    "    * `tokenize (bool, optional)`: This argument is likely set to `False` here, indicating that the tokenizer should not be applied at this stage. The conversation history might already be pre-processed text.\n",
    "    * `add_generation_prompt (bool, optional)`: This argument is likely set to `True`, instructing the function to add a prompt at the beginning specifically designed for chat-like text generation tasks. The exact format of this prompt might be specific to the pipeline implementation.\n",
    "\n",
    "**2. Text Generation with Pipeline (`pipeline`)**\n",
    "\n",
    "* **Function:** This line calls the text-generation pipeline itself to generate text that continues the conversation based on the provided prompt.\n",
    "* **Arguments:**\n",
    "    * `prompt (str)`: This argument is the crucial prompt created in the previous step, containing the formatted conversation history and potentially a generation prompt.\n",
    "    * `max_new_tokens (int, optional)`: This argument limits the number of tokens (words) the LLM can generate to 512 in this case.\n",
    "    * `add_special_tokens (bool, optional)`: This argument, set to `True` here, instructs the pipeline to add special tokens (like start/end of sequence tokens) to the prompt before feeding it to the LLM.\n",
    "    * `do_sample (bool, optional)`: Set to `True` here, enabling random sampling during generation, which can introduce some variation in the response compared to greedy decoding.\n",
    "    * `temperature (float, optional)`: This argument controls the randomness of the generated text. A temperature of 1.0 samples more uniformly from all possibilities, while lower values like 0.7 (used here) favor higher probability tokens, resulting in a more conservative response.\n",
    "    * `top_k (int, optional)`: This argument restricts the generation to only consider the top k most likely tokens at each step, potentially reducing the likelihood of going off on tangents. Here, it's set to 10.\n",
    "    * `top_p (float, optional)`: This argument controls the sampling process by favoring the top p probability mass of the distribution (considering the top k tokens). A value of 0.95 (used here) indicates a preference for high-probability tokens, influencing the creativity of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:57:26.055633Z",
     "iopub.status.busy": "2025-04-01T14:57:26.055373Z",
     "iopub.status.idle": "2025-04-01T14:57:26.072043Z",
     "shell.execute_reply": "2025-04-01T14:57:26.071249Z",
     "shell.execute_reply.started": "2025-04-01T14:57:26.055603Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # question = \"What is the difference between a variable and an object\"\n",
    "# system =  \"\"You are a friendly tutor who explains programming to 10-year-olds using emojis.Keep explanations simple, use relatable examples, and add a cheerful tone. \"\n",
    "# question =system + \"What is the difference between a variable and an object\"\n",
    "# message = [\n",
    "#     {\"role\": \"user\", \"content\": question},\n",
    "# ]\n",
    "\n",
    "# prompt = pipeline.tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# outputs = pipeline(\n",
    "#     prompt,\n",
    "#     max_new_tokens=512,\n",
    "#     add_special_tokens=True,\n",
    "#     do_sample=True,\n",
    "#     temperature=0.7,\n",
    "#     top_k=10,\n",
    "#     top_p=0.95\n",
    "# )\n",
    "# Markdown(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code below combines a large language model (LLM) for text generation with a retrieval system for building a more informative response system. Here's a breakdown of what each part does:\n",
    "\n",
    "**1. Creating a Custom LLM Pipeline (`gemma_llm`)**\n",
    "\n",
    "* **HuggingFacePipeline:** This line likely wraps the existing text-generation pipeline (`pipeline`) from the previous steps into a custom `HuggingFacePipeline` class. This might provide additional functionalities or a more convenient way to interact with the pipeline.\n",
    "* **Model Kwargs:** It sets a default argument (`temperature=0.7`) for the custom pipeline. This argument controls the randomness of the generated text during text generation (explained earlier).\n",
    "\n",
    "**2. Building a RetrievalQA Object (`qa`)**\n",
    "\n",
    "* **RetrievalQA.from_chain_type:** This line creates a `RetrievalQA` object, likely from a custom library or framework that combines retrieval and question answering functionalities.\n",
    "* **Arguments:**\n",
    "    * `llm (TextGenerationPipeline)`: This argument takes the `gemma_llm` object, which provides the text-generation capabilities.\n",
    "    * `chain_type (str)`: This argument is set to `\"stuff\"`, which might be a specific type of retrieval-LM chain supported by the `RetrievalQA` class. The exact functionality of `\"stuff\"` depends on the library's implementation.\n",
    "    * `retriever (RetrievalInterface)`: This argument likely refers to a separate `retriever` object (not shown here) that handles retrieving relevant information from a knowledge base or document store based on the user's query.\n",
    "\n",
    "**In essence:**\n",
    "\n",
    "This code combines the text-generation capabilities of the LLM with a retrieval system. The `RetrievalQA` object likely leverages the retrieved information (through the `retriever` object) to inform the LLM's text generation process, potentially leading to more comprehensive and informative responses to user queries. The specific details of how retrieval and text generation are chained together depend on the `RetrievalQA` implementation and the `\"stuff\"` chain type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:57:26.073119Z",
     "iopub.status.busy": "2025-04-01T14:57:26.072885Z",
     "iopub.status.idle": "2025-04-01T14:57:26.086646Z",
     "shell.execute_reply": "2025-04-01T14:57:26.085819Z",
     "shell.execute_reply.started": "2025-04-01T14:57:26.073094Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-5e786cdfe137>:1: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  gemma_llm = HuggingFacePipeline(\n"
     ]
    }
   ],
   "source": [
    "gemma_llm = HuggingFacePipeline(\n",
    "    pipeline=pipeline,\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"add_special_tokens\": True,\n",
    "        \"do_sample\": True,\n",
    "        \"top_k\": 10,\n",
    "        \"top_p\": 0.95\n",
    "    },\n",
    ")\n",
    "# Create a RetrievalQA object\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=gemma_llm,  # Pass the text-generation pipeline object\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever  # retriever object\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:57:26.087814Z",
     "iopub.status.busy": "2025-04-01T14:57:26.087531Z",
     "iopub.status.idle": "2025-04-01T14:57:26.098640Z",
     "shell.execute_reply": "2025-04-01T14:57:26.097761Z",
     "shell.execute_reply.started": "2025-04-01T14:57:26.087786Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def askQuestion(question):\n",
    "    \n",
    "    system =  \"You are a friendly tutor who explains programming to 10-year-olds using and emojis.Keep explanations simple, use relatable examples, and add a cheerful tone. \"\n",
    "    question = system + question    \n",
    "    message = [\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "    \n",
    "    prompt = pipeline.tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True, truncation=True)\n",
    "    result = qa.invoke(prompt)\n",
    "    display(Markdown(result['result'].split('Helpful Answer:')[1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:57:26.099884Z",
     "iopub.status.busy": "2025-04-01T14:57:26.099596Z",
     "iopub.status.idle": "2025-04-01T14:57:53.268406Z",
     "shell.execute_reply": "2025-04-01T14:57:53.267522Z",
     "shell.execute_reply.started": "2025-04-01T14:57:26.099857Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Hey there! \n",
       "\n",
       "Let's take a look at your code and see what's going on. \n",
       "\n",
       "The function is called `is_prime` and it takes a single argument, `n`.\n",
       "\n",
       "The function checks if the number `n` is a prime number. It does this by iterating through all the numbers from 2 to `n-1`.\n",
       "\n",
       "Inside the for loop, it checks if `n` is divisible by any of the numbers in that range. If it finds a divisor, it returns `False`.\n",
       "\n",
       "However, there's a small issue in the code. It returns `True` for `n` = 1, which is not a prime number.\n",
       "\n",
       "So, the function is not working correctly for numbers less than 2.\n",
       "\n",
       "Here's the corrected code:\n",
       "\n",
       "```python\n",
       "def is_prime(n):\n",
       "    if n <= 1:\n",
       "        return False\n",
       "    for i in range(2, int(n**0.5) + 1):\n",
       "        if n % i == 0:\n",
       "            return False\n",
       "    return True\n",
       "```\n",
       "\n",
       "Now, let's see how it works:\n",
       "\n",
       "1. The function first checks if `n` is less than or equal to 1. If it is, it returns `False` because 1 is not considered a prime number.\n",
       "\n",
       "2. If `n` is greater than 1 and not divisible by any numbers in the range from 2 to its square root, it means `n` is a prime number.\n",
       "\n",
       "3. If it finds a divisor during the for loop, it returns `False`.\n",
       "\n",
       "4. If the for loop completes without finding any divisors, it returns `True`, indicating that `n` is a prime number.\n",
       "\n",
       "With these changes, the function should work correctly for all positive integers, including prime numbers. "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ques='''I wrote a Python function to check if a number is prime, but it's not working correctly. Heres my code:\"\n",
    "    \n",
    "    def is_prime(n):\n",
    "        for i in range(2, n):\n",
    "            if n % i == 0:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    print(is_prime(1))  # Expected: False\n",
    "    print(is_prime(2))  # Expected: True\n",
    "    print(is_prime(4))  # Expected: False\n",
    "    \"It returns True for 1, but 1 is not a prime number. Whats wrong?'''\n",
    "askQuestion(ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:57:53.272031Z",
     "iopub.status.busy": "2025-04-01T14:57:53.271805Z",
     "iopub.status.idle": "2025-04-01T14:58:08.344320Z",
     "shell.execute_reply": "2025-04-01T14:58:08.343395Z",
     "shell.execute_reply.started": "2025-04-01T14:57:53.272012Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "The bug in the code is that the condition in the lambda function is checking if the number is divisible by 2. However, the code is supposed to return only the odd numbers from the list. To fix this, we need to change the condition to check if the number is odd.\n",
       "\n",
       "Here's the corrected code:\n",
       "\n",
       "```python\n",
       "numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
       "filtered_numbers = filter(lambda x: x % 2!= 0, numbers)\n",
       "print(list(filtered_numbers))  # Expected output: [1, 3, 5, 7, 9]\n",
       "```\n",
       "\n",
       "In this corrected code, we use the lambda function to filter the numbers list. The condition now checks if the number is odd (represented by the condition `x % 2!= 0`) and adds the numbers to the `filtered_numbers` list."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q='''\n",
    "\n",
    "    numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    filtered_numbers = filter(lambda x: x % 2 == 0, numbers)\n",
    "    print(list(filtered_numbers))  # Expected output: [1, 3, 5, 7, 9]\n",
    "    Question:\n",
    "    Identify the bug in the code.\n",
    "    Fix the bug so that the function correctly returns only the odd numbers.\n",
    "\n",
    "'''\n",
    "askQuestion(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:58:08.346030Z",
     "iopub.status.busy": "2025-04-01T14:58:08.345692Z",
     "iopub.status.idle": "2025-04-01T14:58:28.317979Z",
     "shell.execute_reply": "2025-04-01T14:58:28.317259Z",
     "shell.execute_reply.started": "2025-04-01T14:58:08.345996Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " \"Hey there!  Let's say you have a list of things, like a yummy fruit basket. To take out the middle item, we can use a special method called'middle'. It's like a magician's trick where we move things around a bit.\n",
       "\n",
       "Here's how it works:\n",
       "\n",
       "1. First, we use the 'insert' method to add a new item at the end of the list. Think of it like adding a new toy to the middle of the fruit basket.\n",
       "\n",
       "2. Then, we use the'remove' method to take the middle item out. It's like taking out a toy from the middle of the basket.\n",
       "\n",
       "3. If you want to remove the entire middle item, we can use the 'del' statement. It's like a magician's trick where we remove a toy from the middle of the basket.\n",
       "\n",
       "4. If you want to change the order of the items in the list, we can use the'swap' method. It's like a magician's trick where we rearrange toys in the basket.\n",
       "\n",
       "So, to take out the middle item, we use the'middle' method, and to remove the entire middle item, we use the 'del' statement. It's like a magician's trick where we rearrange the toys in the basket and then remove one of them!\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "q='''okay so if I have a list and I wanna like take out the middle thing but I don't know how many things there are what do I do?'''\n",
    "askQuestion(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:58:28.319202Z",
     "iopub.status.busy": "2025-04-01T14:58:28.318881Z",
     "iopub.status.idle": "2025-04-01T14:58:40.867120Z",
     "shell.execute_reply": "2025-04-01T14:58:40.866259Z",
     "shell.execute_reply.started": "2025-04-01T14:58:28.319168Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Sure, here's a simplified explanation of how Python can make your computer talk:\n",
       "\n",
       "\"Python can make your computer talk by using a special language called **Natural Language Processing (NLP)**. NLP is like a translator that can understand human language and convert it into code.\n",
       "\n",
       "When you write Python code, you're essentially telling the computer to do something. For example, you could tell Python to:\n",
       "\n",
       "* Open a new window\n",
       "* Write a song\n",
       "* Play a game\n",
       "* Read a book\n",
       "\n",
       "When you run the code, Python will use its NLP capabilities to understand what you've written and then carry out the instructions.\n",
       "\n",
       "So, while Python itself doesn't directly \"talk to\" your computer, it can be used to control it and make it do things that would otherwise be difficult or impossible for a human to do."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q=\"okay but can Python make my computer talk?\"\n",
    "askQuestion(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:58:40.868294Z",
     "iopub.status.busy": "2025-04-01T14:58:40.867999Z",
     "iopub.status.idle": "2025-04-01T14:58:58.469083Z",
     "shell.execute_reply": "2025-04-01T14:58:58.468324Z",
     "shell.execute_reply.started": "2025-04-01T14:58:40.868263Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Hey there! \n",
       "\n",
       "Let's work together to understand why your turtle isn't moving. \n",
       "\n",
       "First, let's check if the turtle has been initialized properly. \n",
       "\n",
       "**Step 1: Checking if the turtle is alive**\n",
       "\n",
       "Like a friendly shopkeeper checking if the batteries are inserted correctly, make sure the turtle is connected to the power source. \n",
       "\n",
       "**Step 2: Checking if the turtle is moving**\n",
       "\n",
       "Hold down the \"spacebar\" key.  Did you hear the turtle move? \n",
       "\n",
       "**Step 3: Checking if the turtle is listening**\n",
       "\n",
       "Is the turtle looking for instructions? \n",
       "\n",
       "**Step 4: Checking if the turtle is facing the right direction**\n",
       "\n",
       "Make sure the turtle is facing the direction you're telling it to move in. \n",
       "\n",
       "**Step 5: Checking for any errors**\n",
       "\n",
       "Is the turtle plugged in properly? Is the power source turned on? Are there any errors in the code?\n",
       "\n",
       "Remember, my friend, troubleshooting takes time and patience. Don't get discouraged if you don't get it right away. Keep trying different things and asking for help from a friendly tutor like me! "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q=\"why is my turtle not moving I said forward but it's just sitting there??\"\n",
    "askQuestion(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T14:06:45.407233Z",
     "iopub.status.busy": "2025-03-30T14:06:45.406943Z",
     "iopub.status.idle": "2025-03-30T14:06:54.535686Z",
     "shell.execute_reply": "2025-03-30T14:06:54.534601Z",
     "shell.execute_reply.started": "2025-03-30T14:06:45.407212Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_gemma/tokenizer_config.json',\n",
       " './fine_tuned_gemma/special_tokens_map.json',\n",
       " './fine_tuned_gemma/tokenizer.model',\n",
       " './fine_tuned_gemma/added_tokens.json',\n",
       " './fine_tuned_gemma/tokenizer.json')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = \"./fine_tuned_gemma\"\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:59:07.668253Z",
     "iopub.status.busy": "2025-04-01T14:59:07.667855Z",
     "iopub.status.idle": "2025-04-01T14:59:07.780886Z",
     "shell.execute_reply": "2025-04-01T14:59:07.779949Z",
     "shell.execute_reply.started": "2025-04-01T14:59:07.668212Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:59:07.782109Z",
     "iopub.status.busy": "2025-04-01T14:59:07.781851Z",
     "iopub.status.idle": "2025-04-01T14:59:55.826914Z",
     "shell.execute_reply": "2025-04-01T14:59:55.826048Z",
     "shell.execute_reply.started": "2025-04-01T14:59:07.782081Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1faed6e0a9354ab4b0115564895accfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged model saved to ./merged_gemma\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Load the base model (Gemma-2B-it)\n",
    "base_model_path = model_path  # Already defined as the snapshot download path\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_path, device_map=\"auto\")\n",
    "\n",
    "# Load the fine-tuned LoRA model\n",
    "lora_model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "\n",
    "# Merge the LoRA adapters with the base model\n",
    "merged_model = lora_model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "merged_output_dir = \"./merged_gemma\"\n",
    "merged_model.save_pretrained(merged_output_dir)\n",
    "tokenizer.save_pretrained(merged_output_dir)\n",
    "print(f\"Merged model saved to {merged_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:59:55.827916Z",
     "iopub.status.busy": "2025-04-01T14:59:55.827679Z",
     "iopub.status.idle": "2025-04-01T15:01:39.992730Z",
     "shell.execute_reply": "2025-04-01T15:01:39.991582Z",
     "shell.execute_reply.started": "2025-04-01T14:59:55.827897Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n",
      "remote: Enumerating objects: 47520, done.\u001b[K\n",
      "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
      "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
      "remote: Total 47520 (delta 12), reused 5 (delta 5), pack-reused 47492 (from 3)\u001b[K\n",
      "Receiving objects: 100% (47520/47520), 99.42 MiB | 30.22 MiB/s, done.\n",
      "Resolving deltas: 100% (34154/34154), done.\n",
      "/kaggle/working/llama.cpp\n",
      "Makefile:2: *** The Makefile build is deprecated. Use the CMake build instead. For more details, see https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md.  Stop.\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: numpy~=1.26.4 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements-convert_legacy_llama.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: sentencepiece~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements-convert_legacy_llama.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.45.1 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.47.0)\n",
      "Collecting gguf>=0.1.0 (from -r ./requirements/requirements-convert_legacy_llama.txt (line 4))\n",
      "  Downloading gguf-0.14.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting protobuf<5.0.0,>=4.21.0 (from -r ./requirements/requirements-convert_legacy_llama.txt (line 5))\n",
      "  Downloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting torch~=2.2.1 (from -r ./requirements/requirements-convert_hf_to_gguf.txt (line 3))\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.2.2%2Bcpu-cp310-cp310-linux_x86_64.whl (186.8 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m186.8/186.8 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting aiohttp~=3.9.3 (from -r ./requirements/requirements-tool_bench.txt (line 1))\n",
      "  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: pytest~=8.3.3 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 2)) (8.3.4)\n",
      "Collecting huggingface_hub~=0.23.2 (from -r ./requirements/requirements-tool_bench.txt (line 3))\n",
      "  Downloading huggingface_hub-0.23.5-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting matplotlib~=3.10.0 (from -r ./requirements/requirements-tool_bench.txt (line 4))\n",
      "  Downloading matplotlib-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting openai~=1.55.3 (from -r ./requirements/requirements-tool_bench.txt (line 6))\n",
      "  Downloading openai-1.55.3-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: pandas~=2.2.3 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 7)) (2.2.3)\n",
      "Collecting prometheus-client~=0.20.0 (from -r ./requirements/requirements-tool_bench.txt (line 8))\n",
      "  Downloading prometheus_client-0.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: requests~=2.32.3 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 9)) (2.32.3)\n",
      "Collecting wget~=3.2 (from -r ./requirements/requirements-tool_bench.txt (line 10))\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: typer~=0.15.1 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 11)) (0.15.1)\n",
      "Collecting seaborn~=0.13.2 (from -r ./requirements/requirements-tool_bench.txt (line 12))\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy~=1.26.4->-r ./requirements/requirements-convert_legacy_llama.txt (line 1)) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy~=1.26.4->-r ./requirements/requirements-convert_legacy_llama.txt (line 1)) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy~=1.26.4->-r ./requirements/requirements-convert_legacy_llama.txt (line 1)) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy~=1.26.4->-r ./requirements/requirements-convert_legacy_llama.txt (line 1)) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy~=1.26.4->-r ./requirements/requirements-convert_legacy_llama.txt (line 1)) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy~=1.26.4->-r ./requirements/requirements-convert_legacy_llama.txt (line 1)) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.45.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.17.0)\n",
      "INFO: pip is looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting transformers<5.0.0,>=4.45.1 (from -r ./requirements/requirements-convert_legacy_llama.txt (line 3))\n",
      "  Downloading transformers-4.50.3-py3-none-any.whl.metadata (39 kB)\n",
      "  Downloading transformers-4.50.2-py3-none-any.whl.metadata (39 kB)\n",
      "  Downloading transformers-4.50.1-py3-none-any.whl.metadata (39 kB)\n",
      "  Downloading transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is still looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.48.0-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.45.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 3)) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.45.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 3)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.45.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.11.6)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers<5.0.0,>=4.45.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 3))\n",
      "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.45.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.45.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2024.12.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (1.18.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (4.0.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest~=8.3.3->-r ./requirements/requirements-tool_bench.txt (line 2)) (1.2.2)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest~=8.3.3->-r ./requirements/requirements-tool_bench.txt (line 2)) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest~=8.3.3->-r ./requirements/requirements-tool_bench.txt (line 2)) (1.5.0)\n",
      "Requirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/dist-packages (from pytest~=8.3.3->-r ./requirements/requirements-tool_bench.txt (line 2)) (2.2.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (2.11.0a2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas~=2.2.3->-r ./requirements/requirements-tool_bench.txt (line 7)) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas~=2.2.3->-r ./requirements/requirements-tool_bench.txt (line 7)) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.3->-r ./requirements/requirements-tool_bench.txt (line 9)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.3->-r ./requirements/requirements-tool_bench.txt (line 9)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.3->-r ./requirements/requirements-tool_bench.txt (line 9)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.3->-r ./requirements/requirements-tool_bench.txt (line 9)) (2025.1.31)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (13.9.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (2.29.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (2.19.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (0.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch~=2.2.1->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy~=1.26.4->-r ./requirements/requirements-convert_legacy_llama.txt (line 1)) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy~=1.26.4->-r ./requirements/requirements-convert_legacy_llama.txt (line 1)) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy~=1.26.4->-r ./requirements/requirements-convert_legacy_llama.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy~=1.26.4->-r ./requirements/requirements-convert_legacy_llama.txt (line 1)) (2024.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch~=2.2.1->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy~=1.26.4->-r ./requirements/requirements-convert_legacy_llama.txt (line 1)) (2024.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (0.1.2)\n",
      "Downloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading gguf-0.14.0-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.5-py3-none-any.whl (402 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m402.8/402.8 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.55.3-py3-none-any.whl (389 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading prometheus_client-0.20.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=e7fe3c21b62399a29169f3cf51e420e3aa01a2c6bd62a21432760c1c99a5852b\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
      "Successfully built wget\n",
      "Installing collected packages: wget, protobuf, prometheus-client, torch, huggingface_hub, tokenizers, openai, aiohttp, matplotlib, transformers, seaborn, gguf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.4\n",
      "    Uninstalling protobuf-5.29.4:\n",
      "      Successfully uninstalled protobuf-5.29.4\n",
      "  Attempting uninstall: prometheus-client\n",
      "    Found existing installation: prometheus_client 0.21.1\n",
      "    Uninstalling prometheus_client-0.21.1:\n",
      "      Successfully uninstalled prometheus_client-0.21.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.1+cu121\n",
      "    Uninstalling torch-2.5.1+cu121:\n",
      "      Successfully uninstalled torch-2.5.1+cu121\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.29.0\n",
      "    Uninstalling huggingface-hub-0.29.0:\n",
      "      Successfully uninstalled huggingface-hub-0.29.0\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.0\n",
      "    Uninstalling tokenizers-0.21.0:\n",
      "      Successfully uninstalled tokenizers-0.21.0\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.57.4\n",
      "    Uninstalling openai-1.57.4:\n",
      "      Successfully uninstalled openai-1.57.4\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.11.12\n",
      "    Uninstalling aiohttp-3.11.12:\n",
      "      Successfully uninstalled aiohttp-3.11.12\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.7.5\n",
      "    Uninstalling matplotlib-3.7.5:\n",
      "      Successfully uninstalled matplotlib-3.7.5\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.47.0\n",
      "    Uninstalling transformers-4.47.0:\n",
      "      Successfully uninstalled transformers-4.47.0\n",
      "  Attempting uninstall: seaborn\n",
      "    Found existing installation: seaborn 0.12.2\n",
      "    Uninstalling seaborn-0.12.2:\n",
      "      Successfully uninstalled seaborn-0.12.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 3.3.1 requires huggingface-hub>=0.24.0, but you have huggingface-hub 0.23.5 which is incompatible.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 4.25.6 which is incompatible.\n",
      "google-cloud-bigtable 2.27.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "mlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\n",
      "opentelemetry-proto 1.31.1 requires protobuf<6.0,>=5.0, but you have protobuf 4.25.6 which is incompatible.\n",
      "pandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "peft 0.14.0 requires huggingface-hub>=0.25.0, but you have huggingface-hub 0.23.5 which is incompatible.\n",
      "tensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\n",
      "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.2.2+cpu which is incompatible.\n",
      "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.2.2+cpu which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiohttp-3.9.5 gguf-0.14.0 huggingface_hub-0.23.5 matplotlib-3.10.1 openai-1.55.3 prometheus-client-0.20.0 protobuf-4.25.6 seaborn-0.13.2 tokenizers-0.20.3 torch-2.2.2+cpu transformers-4.46.3 wget-3.2\n"
     ]
    }
   ],
   "source": [
    "# Install llama.cpp and its Python bindings\n",
    "!git clone https://github.com/ggerganov/llama.cpp.git\n",
    "%cd llama.cpp\n",
    "!make\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T15:01:39.994389Z",
     "iopub.status.busy": "2025-04-01T15:01:39.994131Z",
     "iopub.status.idle": "2025-04-01T15:01:47.098071Z",
     "shell.execute_reply": "2025-04-01T15:01:47.097055Z",
     "shell.execute_reply.started": "2025-04-01T15:01:39.994367Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
      "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
      "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]                           \n",
      "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease                                              \n",
      "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]                                \n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]                             \n",
      "Get:7 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [69.9 kB]       \n",
      "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,381 kB]\n",
      "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]              \n",
      "Get:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
      "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,685 kB]\n",
      "Get:14 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [47.7 kB]\n",
      "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,788 kB]              \n",
      "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,773 kB]              \n",
      "Get:17 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [33.6 kB]   \n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,081 kB]                \n",
      "Get:19 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [46.8 kB]\n",
      "Get:20 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,972 kB]        \n",
      "Get:21 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,241 kB]       \n",
      "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [55.7 kB]       \n",
      "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,540 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,126 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [82.7 kB]\n",
      "Fetched 30.4 MB in 3s (11.2 MB/s)                            \n",
      "Reading package lists... Done\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "build-essential is already the newest version (12.9ubuntu3).\n",
      "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 175 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt-get update\n",
    "!apt-get install -y cmake build-essential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T15:30:41.814033Z",
     "iopub.status.busy": "2025-04-01T15:30:41.813640Z",
     "iopub.status.idle": "2025-04-01T15:36:39.991970Z",
     "shell.execute_reply": "2025-04-01T15:36:39.990978Z",
     "shell.execute_reply.started": "2025-04-01T15:30:41.814007Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/llama.cpp/build\n",
      "-- The C compiler identification is GNU 11.4.0\n",
      "-- The CXX compiler identification is GNU 11.4.0\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /usr/bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "-- Found Threads: TRUE\n",
      "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
      "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "-- Including CPU backend\n",
      "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
      "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
      "-- Found OpenMP: TRUE (found version \"4.5\")\n",
      "-- x86 detected\n",
      "-- Adding CPU backend variant ggml-cpu: -march=native \n",
      "-- Configuring done (2.1s)\n",
      "-- Generating done (0.2s)\n",
      "-- Build files have been written to: /kaggle/working/llama.cpp/build\n",
      "[  0%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
      "[  0%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
      "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
      "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
      "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
      "[  2%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
      "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
      "[  3%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
      "[  3%] Built target ggml-base\n",
      "[  4%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
      "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
      "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o\u001b[0m\n",
      "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o\u001b[0m\n",
      "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
      "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
      "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
      "[  9%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
      "[  9%] Built target ggml-cpu\n",
      "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
      "[ 10%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n",
      "[ 10%] Built target ggml\n",
      "[ 10%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
      "[ 20%] Built target llama\n",
      "[ 20%] \u001b[34m\u001b[1mGenerating build details from Git\u001b[0m\n",
      "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
      "[ 20%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
      "[ 20%] Built target build_info\n",
      "[ 21%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
      "[ 26%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
      "[ 26%] Built target common\n",
      "[ 27%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
      "[ 27%] Built target test-tokenizer-0\n",
      "[ 27%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
      "[ 28%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
      "[ 28%] Built target test-sampling\n",
      "[ 28%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
      "[ 29%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
      "[ 29%] Built target test-grammar-parser\n",
      "[ 30%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
      "[ 30%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
      "[ 31%] Built target test-grammar-integration\n",
      "[ 32%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
      "[ 33%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
      "[ 33%] Built target test-llama-grammar\n",
      "[ 33%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n",
      "[ 34%] Built target test-chat\n",
      "[ 34%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
      "[ 35%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
      "[ 35%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
      "[ 35%] Built target test-json-schema-to-grammar\n",
      "[ 36%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
      "[ 36%] Built target test-tokenizer-1-bpe\n",
      "[ 37%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
      "[ 37%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
      "[ 37%] Built target test-tokenizer-1-spm\n",
      "[ 37%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n",
      "[ 38%] Built target test-log\n",
      "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
      "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
      "[ 40%] Built target test-arg-parser\n",
      "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
      "[ 42%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
      "[ 42%] Built target test-chat-template\n",
      "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n",
      "[ 43%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n",
      "[ 43%] Built target test-gguf\n",
      "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
      "[ 45%] Built target test-backend-ops\n",
      "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
      "[ 47%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
      "[ 47%] Built target test-model-load-cancel\n",
      "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
      "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
      "[ 48%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
      "[ 48%] Built target test-autorelease\n",
      "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
      "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
      "[ 49%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
      "[ 49%] Built target test-barrier\n",
      "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
      "[ 50%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
      "[ 50%] Built target test-quantize-fns\n",
      "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
      "[ 51%] Built target test-quantize-perf\n",
      "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
      "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
      "[ 53%] Built target test-rope\n",
      "[ 54%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
      "[ 54%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n",
      "[ 54%] Built target test-c\n",
      "[ 55%] \u001b[32mBuilding CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
      "[ 55%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
      "[ 55%] Built target llama-batched-bench\n",
      "[ 56%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
      "[ 56%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
      "[ 56%] Built target llama-batched\n",
      "[ 56%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
      "[ 57%] Built target llama-embedding\n",
      "[ 57%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
      "[ 58%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
      "[ 58%] Built target llama-eval-callback\n",
      "[ 58%] \u001b[32mBuilding CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o\u001b[0m\n",
      "[ 58%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gbnf-validator\u001b[0m\n",
      "[ 58%] Built target llama-gbnf-validator\n",
      "[ 58%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n",
      "[ 58%] Built target sha256\n",
      "[ 59%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n",
      "[ 59%] Built target xxhash\n",
      "[ 59%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n",
      "[ 59%] Built target sha1\n",
      "[ 60%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n",
      "[ 60%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n",
      "[ 60%] Built target llama-gguf-hash\n",
      "[ 61%] \u001b[32mBuilding CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
      "[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
      "[ 61%] Built target llama-gguf-split\n",
      "[ 62%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n",
      "[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n",
      "[ 62%] Built target llama-gguf\n",
      "[ 63%] \u001b[32mBuilding CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o\u001b[0m\n",
      "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gritlm\u001b[0m\n",
      "[ 63%] Built target llama-gritlm\n",
      "[ 64%] \u001b[32mBuilding CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
      "[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
      "[ 64%] Built target llama-imatrix\n",
      "[ 64%] \u001b[32mBuilding CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o\u001b[0m\n",
      "[ 65%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-infill\u001b[0m\n",
      "[ 65%] Built target llama-infill\n",
      "[ 65%] \u001b[32mBuilding CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
      "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
      "[ 66%] Built target llama-bench\n",
      "[ 66%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
      "[ 67%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
      "[ 67%] Built target llama-lookahead\n",
      "[ 67%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
      "[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
      "[ 68%] Built target llama-lookup\n",
      "[ 68%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
      "[ 69%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
      "[ 69%] Built target llama-lookup-create\n",
      "[ 69%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
      "[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
      "[ 70%] Built target llama-lookup-merge\n",
      "[ 70%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
      "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
      "[ 71%] Built target llama-lookup-stats\n",
      "[ 71%] \u001b[32mBuilding CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
      "[ 72%] Built target llama-cli\n",
      "[ 73%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
      "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
      "[ 73%] Built target llama-parallel\n",
      "[ 74%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
      "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
      "[ 74%] Built target llama-passkey\n",
      "[ 75%] \u001b[32mBuilding CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
      "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
      "[ 75%] Built target llama-perplexity\n",
      "[ 76%] \u001b[32mBuilding CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
      "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
      "[ 76%] Built target llama-quantize\n",
      "[ 76%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
      "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
      "[ 77%] Built target llama-retrieval\n",
      "[ 77%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n",
      "[ 78%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\n",
      "[ 79%] \u001b[32mBuilding CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
      "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
      "[ 79%] Built target llama-server\n",
      "[ 80%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
      "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
      "[ 80%] Built target llama-save-load-state\n",
      "[ 80%] \u001b[32mBuilding CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o\u001b[0m\n",
      "[ 81%] \u001b[32mBuilding CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o\u001b[0m\n",
      "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-run\u001b[0m\n",
      "[ 81%] Built target llama-run\n",
      "[ 82%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
      "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
      "[ 82%] Built target llama-simple\n",
      "[ 83%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
      "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
      "[ 83%] Built target llama-simple-chat\n",
      "[ 83%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
      "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
      "[ 84%] Built target llama-speculative\n",
      "[ 84%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n",
      "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n",
      "[ 85%] Built target llama-speculative-simple\n",
      "[ 85%] \u001b[32mBuilding CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
      "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
      "[ 86%] Built target llama-tokenize\n",
      "[ 86%] \u001b[32mBuilding CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n",
      "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n",
      "[ 87%] Built target llama-tts\n",
      "[ 88%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n",
      "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n",
      "[ 88%] Built target llama-gen-docs\n",
      "[ 88%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
      "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
      "[ 89%] Built target llama-convert-llama2c-to-ggml\n",
      "[ 89%] \u001b[32mBuilding CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
      "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
      "[ 90%] Built target llama-cvector-generator\n",
      "[ 90%] \u001b[32mBuilding CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
      "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
      "[ 91%] Built target llama-export-lora\n",
      "[ 92%] \u001b[32mBuilding CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o\u001b[0m\n",
      "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize-stats\u001b[0m\n",
      "[ 92%] Built target llama-quantize-stats\n",
      "[ 92%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o\u001b[0m\n",
      "[ 93%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o\u001b[0m\n",
      "[ 93%] Built target llava\n",
      "[ 94%] \u001b[32m\u001b[1mLinking CXX static library libllava_static.a\u001b[0m\n",
      "[ 94%] Built target llava_static\n",
      "[ 94%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libllava_shared.so\u001b[0m\n",
      "[ 94%] Built target llava_shared\n",
      "[ 94%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o\u001b[0m\n",
      "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
      "[ 95%] Built target llama-llava-cli\n",
      "[ 95%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o\u001b[0m\n",
      "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
      "[ 95%] Built target llama-minicpmv-cli\n",
      "[ 96%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o\u001b[0m\n",
      "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n",
      "[ 96%] Built target llama-qwen2vl-cli\n",
      "[ 97%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-gemma3-cli.dir/gemma3-cli.cpp.o\u001b[0m\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\n",
      "[ 97%] Built target llama-gemma3-cli\n",
      "[ 97%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o\u001b[0m\n",
      "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-clip-quantize-cli\u001b[0m\n",
      "[ 98%] Built target llama-llava-clip-quantize-cli\n",
      "[ 98%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
      "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
      "[ 99%] Built target llama-vdot\n",
      "[100%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
      "[100%] Built target llama-q8dot\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p build\n",
    "%cd build\n",
    "!cmake ..\n",
    "!cmake --build . --config Release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T15:45:29.906618Z",
     "iopub.status.busy": "2025-04-01T15:45:29.906226Z",
     "iopub.status.idle": "2025-04-01T15:47:32.236661Z",
     "shell.execute_reply": "2025-04-01T15:47:32.235585Z",
     "shell.execute_reply.started": "2025-04-01T15:45:29.906584Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: numpy~=1.26.4 in /usr/local/lib/python3.10/dist-packages (from -r .././requirements/requirements-convert_legacy_llama.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: sentencepiece~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r .././requirements/requirements-convert_legacy_llama.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.45.1 in /usr/local/lib/python3.10/dist-packages (from -r .././requirements/requirements-convert_legacy_llama.txt (line 3)) (4.46.3)\n",
      "Requirement already satisfied: gguf>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from -r .././requirements/requirements-convert_legacy_llama.txt (line 4)) (0.14.0)\n",
      "Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /usr/local/lib/python3.10/dist-packages (from -r .././requirements/requirements-convert_legacy_llama.txt (line 5)) (4.25.6)\n",
      "Requirement already satisfied: torch~=2.2.1 in /usr/local/lib/python3.10/dist-packages (from -r .././requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2.2.2+cpu)\n",
      "Requirement already satisfied: aiohttp~=3.9.3 in /usr/local/lib/python3.10/dist-packages (from -r .././requirements/requirements-tool_bench.txt (line 1)) (3.9.5)\n",
      "Requirement already satisfied: pytest~=8.3.3 in /usr/local/lib/python3.10/dist-packages (from -r .././requirements/requirements-tool_bench.txt (line 2)) (8.3.4)\n",
      "Requirement already satisfied: huggingface_hub~=0.23.2 in /usr/local/lib/python3.10/dist-packages (from -r .././requirements/requirements-tool_bench.txt (line 3)) (0.23.5)\n",
      "Requirement already satisfied: matplotlib~=3.10.0 in /usr/local/lib/python3.10/dist-packages (from -r .././requirements/requirements-tool_bench.txt (line 4)) (3.10.1)\n",
      "Requirement already satisfied: openai~=1.55.3 in /usr/local/lib/python3.10/dist-packages (from -r .././requirements/requirements-tool_bench.txt (line 6)) (1.55.3)\n",
      "Requirement already satisfied: pandas~=2.2.3 in /usr/local/lib/python3.10/dist-packages (from -r .././requirements/requirements-tool_bench.txt (line 7)) (2.2.3)\n",
      "Requirement already satisfied: prometheus-client~=0.20.0 in /usr/local/lib/python3.10/dist-packages (from -r .././requirements/requirements-tool_bench.txt (line 8)) (0.20.0)\n",
      "Requirement already satisfied: requests~=2.32.3 in /usr/local/lib/python3.10/dist-packages (from -r .././requirements/requirements-tool_bench.txt (line 9)) (2.32.3)\n",
      "Requirement already satisfied: wget~=3.2 in /usr/local/lib/python3.10/dist-packages (from -r .././requirements/requirements-tool_bench.txt (line 10)) (3.2)\n",
      "Requirement already satisfied: typer~=0.15.1 in /usr/local/lib/python3.10/dist-packages (from -r .././requirements/requirements-tool_bench.txt (line 11)) (0.15.1)\n",
      "Requirement already satisfied: seaborn~=0.13.2 in /usr/local/lib/python3.10/dist-packages (from -r .././requirements/requirements-tool_bench.txt (line 12)) (0.13.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy~=1.26.4->-r .././requirements/requirements-convert_legacy_llama.txt (line 1)) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy~=1.26.4->-r .././requirements/requirements-convert_legacy_llama.txt (line 1)) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy~=1.26.4->-r .././requirements/requirements-convert_legacy_llama.txt (line 1)) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy~=1.26.4->-r .././requirements/requirements-convert_legacy_llama.txt (line 1)) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy~=1.26.4->-r .././requirements/requirements-convert_legacy_llama.txt (line 1)) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy~=1.26.4->-r .././requirements/requirements-convert_legacy_llama.txt (line 1)) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.45.1->-r .././requirements/requirements-convert_legacy_llama.txt (line 3)) (3.17.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.45.1->-r .././requirements/requirements-convert_legacy_llama.txt (line 3)) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.45.1->-r .././requirements/requirements-convert_legacy_llama.txt (line 3)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.45.1->-r .././requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.45.1->-r .././requirements/requirements-convert_legacy_llama.txt (line 3)) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.45.1->-r .././requirements/requirements-convert_legacy_llama.txt (line 3)) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.45.1->-r .././requirements/requirements-convert_legacy_llama.txt (line 3)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1->-r .././requirements/requirements-convert_hf_to_gguf.txt (line 3)) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1->-r .././requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1->-r .././requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1->-r .././requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1->-r .././requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2024.12.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.9.3->-r .././requirements/requirements-tool_bench.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.9.3->-r .././requirements/requirements-tool_bench.txt (line 1)) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.9.3->-r .././requirements/requirements-tool_bench.txt (line 1)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.9.3->-r .././requirements/requirements-tool_bench.txt (line 1)) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.9.3->-r .././requirements/requirements-tool_bench.txt (line 1)) (1.18.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.9.3->-r .././requirements/requirements-tool_bench.txt (line 1)) (4.0.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest~=8.3.3->-r .././requirements/requirements-tool_bench.txt (line 2)) (1.2.2)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest~=8.3.3->-r .././requirements/requirements-tool_bench.txt (line 2)) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest~=8.3.3->-r .././requirements/requirements-tool_bench.txt (line 2)) (1.5.0)\n",
      "Requirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/dist-packages (from pytest~=8.3.3->-r .././requirements/requirements-tool_bench.txt (line 2)) (2.2.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.10.0->-r .././requirements/requirements-tool_bench.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.10.0->-r .././requirements/requirements-tool_bench.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.10.0->-r .././requirements/requirements-tool_bench.txt (line 4)) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.10.0->-r .././requirements/requirements-tool_bench.txt (line 4)) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.10.0->-r .././requirements/requirements-tool_bench.txt (line 4)) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.10.0->-r .././requirements/requirements-tool_bench.txt (line 4)) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.10.0->-r .././requirements/requirements-tool_bench.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai~=1.55.3->-r .././requirements/requirements-tool_bench.txt (line 6)) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai~=1.55.3->-r .././requirements/requirements-tool_bench.txt (line 6)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai~=1.55.3->-r .././requirements/requirements-tool_bench.txt (line 6)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai~=1.55.3->-r .././requirements/requirements-tool_bench.txt (line 6)) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai~=1.55.3->-r .././requirements/requirements-tool_bench.txt (line 6)) (2.11.0a2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai~=1.55.3->-r .././requirements/requirements-tool_bench.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas~=2.2.3->-r .././requirements/requirements-tool_bench.txt (line 7)) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas~=2.2.3->-r .././requirements/requirements-tool_bench.txt (line 7)) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.3->-r .././requirements/requirements-tool_bench.txt (line 9)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.3->-r .././requirements/requirements-tool_bench.txt (line 9)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.3->-r .././requirements/requirements-tool_bench.txt (line 9)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.3->-r .././requirements/requirements-tool_bench.txt (line 9)) (2025.1.31)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer~=0.15.1->-r .././requirements/requirements-tool_bench.txt (line 11)) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer~=0.15.1->-r .././requirements/requirements-tool_bench.txt (line 11)) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer~=0.15.1->-r .././requirements/requirements-tool_bench.txt (line 11)) (13.9.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai~=1.55.3->-r .././requirements/requirements-tool_bench.txt (line 6)) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai~=1.55.3->-r .././requirements/requirements-tool_bench.txt (line 6)) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai~=1.55.3->-r .././requirements/requirements-tool_bench.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai~=1.55.3->-r .././requirements/requirements-tool_bench.txt (line 6)) (2.29.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.10.0->-r .././requirements/requirements-tool_bench.txt (line 4)) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer~=0.15.1->-r .././requirements/requirements-tool_bench.txt (line 11)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer~=0.15.1->-r .././requirements/requirements-tool_bench.txt (line 11)) (2.19.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp~=3.9.3->-r .././requirements/requirements-tool_bench.txt (line 1)) (0.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch~=2.2.1->-r .././requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy~=1.26.4->-r .././requirements/requirements-convert_legacy_llama.txt (line 1)) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy~=1.26.4->-r .././requirements/requirements-convert_legacy_llama.txt (line 1)) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy~=1.26.4->-r .././requirements/requirements-convert_legacy_llama.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy~=1.26.4->-r .././requirements/requirements-convert_legacy_llama.txt (line 1)) (2024.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch~=2.2.1->-r .././requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy~=1.26.4->-r .././requirements/requirements-convert_legacy_llama.txt (line 1)) (2024.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer~=0.15.1->-r .././requirements/requirements-tool_bench.txt (line 11)) (0.1.2)\n",
      "main: build = 5021 (e39e727e)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing '../../gemma-2b-finetuned.gguf' to '../../gemma-2b-finetuned-q4_k_m.gguf' as Q4_K_M\n",
      "llama_model_loader: loaded meta data with 29 key-value pairs and 164 tensors from ../../gemma-2b-finetuned.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Merged_Gemma\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 2.5B\n",
      "llama_model_loader: - kv   4:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   5:                     gemma.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   6:                          gemma.block_count u32              = 18\n",
      "llama_model_loader: - kv   7:                  gemma.feed_forward_length u32              = 16384\n",
      "llama_model_loader: - kv   8:                 gemma.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv   9:              gemma.attention.head_count_kv u32              = 1\n",
      "llama_model_loader: - kv  10:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  11:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  12:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  13:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eot_token_id u32              = 107\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   37 tensors\n",
      "llama_model_loader: - type  f16:  127 tensors\n",
      "[   1/ 164]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[   2/ 164]                    token_embd.weight - [ 2048, 256000,     1,     1], type =    f16, converting to q6_K .. size =  1000.00 MiB ->   410.16 MiB\n",
      "[   3/ 164]                  blk.0.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[   4/ 164]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[   5/ 164]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[   6/ 164]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[   7/ 164]                  blk.0.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
      "[   8/ 164]                blk.0.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q6_K .. size =    64.00 MiB ->    26.25 MiB\n",
      "[   9/ 164]                blk.0.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[  10/ 164]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  11/ 164]                  blk.0.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[  12/ 164]                  blk.1.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  13/ 164]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  14/ 164]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  15/ 164]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  16/ 164]                  blk.1.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
      "[  17/ 164]                blk.1.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q6_K .. size =    64.00 MiB ->    26.25 MiB\n",
      "[  18/ 164]                blk.1.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[  19/ 164]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  20/ 164]                  blk.1.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[  21/ 164]                  blk.2.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  22/ 164]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  23/ 164]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  24/ 164]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  25/ 164]                  blk.2.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  26/ 164]                blk.2.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[  27/ 164]                blk.2.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[  28/ 164]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  29/ 164]                  blk.2.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[  30/ 164]                  blk.3.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  31/ 164]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  32/ 164]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  33/ 164]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  34/ 164]                  blk.3.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  35/ 164]                blk.3.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[  36/ 164]                blk.3.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[  37/ 164]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  38/ 164]                  blk.3.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[  39/ 164]                  blk.4.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  40/ 164]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  41/ 164]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  42/ 164]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  43/ 164]                  blk.4.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
      "[  44/ 164]                blk.4.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q6_K .. size =    64.00 MiB ->    26.25 MiB\n",
      "[  45/ 164]                blk.4.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[  46/ 164]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  47/ 164]                  blk.4.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[  48/ 164]                  blk.5.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  49/ 164]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  50/ 164]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  51/ 164]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  52/ 164]                  blk.5.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  53/ 164]                blk.5.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[  54/ 164]                blk.5.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[  55/ 164]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  56/ 164]                  blk.5.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[  57/ 164]                  blk.6.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  58/ 164]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  59/ 164]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  60/ 164]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  61/ 164]                  blk.6.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  62/ 164]                blk.6.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[  63/ 164]                blk.6.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[  64/ 164]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  65/ 164]                  blk.6.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[  66/ 164]                  blk.7.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  67/ 164]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  68/ 164]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  69/ 164]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  70/ 164]                  blk.7.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
      "[  71/ 164]                blk.7.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q6_K .. size =    64.00 MiB ->    26.25 MiB\n",
      "[  72/ 164]                blk.7.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[  73/ 164]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  74/ 164]                  blk.7.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[  75/ 164]                  blk.8.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  76/ 164]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  77/ 164]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  78/ 164]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  79/ 164]                  blk.8.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  80/ 164]                blk.8.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[  81/ 164]                blk.8.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[  82/ 164]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  83/ 164]                  blk.8.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[  84/ 164]                  blk.9.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  85/ 164]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  86/ 164]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  87/ 164]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  88/ 164]                  blk.9.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  89/ 164]                blk.9.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[  90/ 164]                blk.9.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[  91/ 164]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  92/ 164]                  blk.9.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[  93/ 164]                 blk.10.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  94/ 164]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  95/ 164]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  96/ 164]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  97/ 164]                 blk.10.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
      "[  98/ 164]               blk.10.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q6_K .. size =    64.00 MiB ->    26.25 MiB\n",
      "[  99/ 164]               blk.10.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[ 100/ 164]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 101/ 164]                 blk.10.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[ 102/ 164]                 blk.11.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[ 103/ 164]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 104/ 164]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 105/ 164]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 106/ 164]                 blk.11.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[ 107/ 164]               blk.11.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[ 108/ 164]               blk.11.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[ 109/ 164]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 110/ 164]                 blk.11.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[ 111/ 164]                 blk.12.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[ 112/ 164]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 113/ 164]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 114/ 164]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 115/ 164]                 blk.12.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[ 116/ 164]               blk.12.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[ 117/ 164]               blk.12.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[ 118/ 164]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 119/ 164]                 blk.12.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[ 120/ 164]                 blk.13.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[ 121/ 164]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 122/ 164]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 123/ 164]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 124/ 164]                 blk.13.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
      "[ 125/ 164]               blk.13.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q6_K .. size =    64.00 MiB ->    26.25 MiB\n",
      "[ 126/ 164]               blk.13.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[ 127/ 164]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 128/ 164]                 blk.13.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[ 129/ 164]                 blk.14.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[ 130/ 164]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 131/ 164]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 132/ 164]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 133/ 164]                 blk.14.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[ 134/ 164]               blk.14.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[ 135/ 164]               blk.14.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[ 136/ 164]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 137/ 164]                 blk.14.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[ 138/ 164]                 blk.15.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[ 139/ 164]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 140/ 164]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 141/ 164]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 142/ 164]                 blk.15.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
      "[ 143/ 164]               blk.15.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q6_K .. size =    64.00 MiB ->    26.25 MiB\n",
      "[ 144/ 164]               blk.15.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[ 145/ 164]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 146/ 164]                 blk.15.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[ 147/ 164]                 blk.16.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[ 148/ 164]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 149/ 164]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 150/ 164]                 blk.16.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 151/ 164]                 blk.16.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
      "[ 152/ 164]               blk.16.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q6_K .. size =    64.00 MiB ->    26.25 MiB\n",
      "[ 153/ 164]               blk.16.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[ 154/ 164]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 155/ 164]                 blk.16.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[ 156/ 164]                 blk.17.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[ 157/ 164]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 158/ 164]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 159/ 164]                 blk.17.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 160/ 164]                 blk.17.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
      "[ 161/ 164]               blk.17.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q6_K .. size =    64.00 MiB ->    26.25 MiB\n",
      "[ 162/ 164]               blk.17.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "[ 163/ 164]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 164/ 164]                 blk.17.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
      "llama_model_quantize_impl: model size  =  4780.29 MB\n",
      "llama_model_quantize_impl: quant size  =  1548.98 MB\n",
      "\n",
      "main: quantize time = 116040.66 ms\n",
      "main:    total time = 116040.66 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Install Python requirements (for conversion script)\n",
    "!pip install -r ../requirements.txt\n",
    "\n",
    "# Convert merged model to GGUF (if not done)\n",
    "!python ../convert_hf_to_gguf.py ../../merged_gemma --outfile ../../gemma-2b-finetuned.gguf\n",
    "\n",
    "# Quantize (if not done)\n",
    "!./bin/llama-quantize ../../gemma-2b-finetuned.gguf ../../gemma-2b-finetuned-q4_k_m.gguf Q4_K_M\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 7669720,
     "sourceId": 64148,
     "sourceType": "competition"
    },
    {
     "datasetId": 4488812,
     "sourceId": 7691577,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4607812,
     "sourceId": 7856134,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4616621,
     "sourceId": 7970419,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6641629,
     "sourceId": 10715066,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6679238,
     "sourceId": 10767154,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6679268,
     "sourceId": 10767194,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6680051,
     "sourceId": 10768286,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6680279,
     "sourceId": 10768708,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6708962,
     "sourceId": 10808000,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6799210,
     "sourceId": 10934133,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7003944,
     "sourceId": 11215974,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 3533,
     "modelInstanceId": 5388,
     "sourceId": 11372,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 3301,
     "modelInstanceId": 8318,
     "sourceId": 11382,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
